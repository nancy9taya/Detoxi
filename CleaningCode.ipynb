{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\lenovo\\.conda\\envs\\tf-gpu\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\lenovo\\.conda\\envs\\tf-gpu\\lib\\site-packages (from wordcloud) (3.4.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\lenovo\\.conda\\envs\\tf-gpu\\lib\\site-packages (from wordcloud) (8.0.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python38\\site-packages (from wordcloud) (1.19.5)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\lenovo\\.conda\\envs\\tf-gpu\\lib\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lenovo\\.conda\\envs\\tf-gpu\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\lenovo\\.conda\\envs\\tf-gpu\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\lenovo\\.conda\\envs\\tf-gpu\\lib\\site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: six in c:\\users\\lenovo\\.conda\\envs\\tf-gpu\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall absl-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install absl-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "POQpUq40q8k5",
    "outputId": "261c9190-fd57-432b-82ff-c5568e033a11"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io \n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import regexp_tokenize   # module for tokenizing strings\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import nltk \n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "zKq9SfCjqAkT",
    "outputId": "84e19602-4fb0-4648-fd3e-05d6bd4b19d8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1\n",
       "0  0\n",
       "1  0\n",
       "2  0\n",
       "3  0\n",
       "4  0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(r'C:\\Users\\Lenovo\\Desktop\\GP Code\\returns.csv')# change path to csv file\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgvB2kbWqWKh"
   },
   "source": [
    "# Clean Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove any empty row after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tweet'].replace('', np.nan, inplace=True)\n",
    "train_df.dropna(subset=['tweet'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7kn7vX4ox-y"
   },
   "source": [
    "### Remove @ username in Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6hOGd94tsL8n"
   },
   "outputs": [],
   "source": [
    "def remove_at_user(text):\n",
    "\n",
    "    \"\"\"\n",
    "      Remove @username from tweets\n",
    "    \"\"\"\n",
    "#     print(text)\n",
    "    return re.sub(\"@[A-Za-z0-9]+\",\"\",str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "hjhj4Vv5tSzj"
   },
   "outputs": [],
   "source": [
    "train_df[\"text_clean\"] = train_df[\"tweet\"].apply(lambda x: remove_at_user(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b1XThQboDLi"
   },
   "source": [
    "###  Contractions \n",
    "\n",
    "We use the contractions package to expand the contraction in English such as we'll -> we will or we shouldn't've -> we should not have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SdIiCz30q_c0",
    "outputId": "17deb78e-c79a-49bd-e969-4b448168d56f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.0.52-py2.py3-none-any.whl (7.2 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Using cached textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting anyascii\n",
      "  Using cached anyascii-0.2.0-py3-none-any.whl (283 kB)\n",
      "Collecting pyahocorasick\n",
      "  Using cached pyahocorasick-1.4.2-cp38-cp38-win_amd64.whl\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.2.0 contractions-0.0.52 pyahocorasick-1.4.2 textsearch-0.0.21\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Kx5ic9egrlK-"
   },
   "outputs": [],
   "source": [
    "import contractions\n",
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: contractions.fix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you do not deserve to exist \n"
     ]
    }
   ],
   "source": [
    "print(contractions.fix(\"you don't deserve to exist \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHdZgf8Sgoi5"
   },
   "source": [
    "### Repeat the captial words \n",
    "Here we repeat the Captial words to confirm the meaning of this word as the user try to focus on it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "l8zIkM2igy5i",
    "outputId": "b0874d5c-fea1-4d53-f574-526e44eb9026"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\.conda\\envs\\tf-gpu\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: regex in c:\\users\\lenovo\\.conda\\envs\\tf-gpu\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\.conda\\envs\\tf-gpu\\lib\\site-packages (from nltk) (4.61.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\.conda\\envs\\tf-gpu\\lib\\site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\.conda\\envs\\tf-gpu\\lib\\site-packages (from nltk) (7.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1108)>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i don't like chatting with you.</td>\n",
       "      <td>0</td>\n",
       "      <td>i do not like chatting with you.</td>\n",
       "      <td>[i, do, not, like, chatting, with, you, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you do not have friends, cause no body loves you.</td>\n",
       "      <td>1</td>\n",
       "      <td>you do not have friends, because no body loves...</td>\n",
       "      <td>[you, do, not, have, friends, ,, because, no, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you don't deserve to exist.</td>\n",
       "      <td>1</td>\n",
       "      <td>you do not deserve to exist.</td>\n",
       "      <td>[you, do, not, deserve, to, exist, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no one cares about your opinion.</td>\n",
       "      <td>1</td>\n",
       "      <td>no one cares about your opinion.</td>\n",
       "      <td>[no, one, cares, about, your, opinion, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no one respect you.</td>\n",
       "      <td>1</td>\n",
       "      <td>no one respect you.</td>\n",
       "      <td>[no, one, respect, you, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label  \\\n",
       "0                    i don't like chatting with you.      0   \n",
       "1  you do not have friends, cause no body loves you.      1   \n",
       "2                        you don't deserve to exist.      1   \n",
       "3                   no one cares about your opinion.      1   \n",
       "4                                no one respect you.      1   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0                   i do not like chatting with you.   \n",
       "1  you do not have friends, because no body loves...   \n",
       "2                       you do not deserve to exist.   \n",
       "3                   no one cares about your opinion.   \n",
       "4                                no one respect you.   \n",
       "\n",
       "                                           tokenized  \n",
       "0         [i, do, not, like, chatting, with, you, .]  \n",
       "1  [you, do, not, have, friends, ,, because, no, ...  \n",
       "2              [you, do, not, deserve, to, exist, .]  \n",
       "3          [no, one, cares, about, your, opinion, .]  \n",
       "4                         [no, one, respect, you, .]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "train_df['tokenized'] = train_df[\"text_clean\"].apply(word_tokenize)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "f7BqLRrIIaoa"
   },
   "outputs": [],
   "source": [
    "train_df['text_clean']=train_df['tokenized'].apply(lambda x:  [word  if word.isupper() == False else word+\" \"+word  for word in x]) \n",
    "train_df['text_clean'] = [' '.join(map(str, l)) for l in train_df['text_clean']] # join back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "XSid1k5Tl267",
    "outputId": "5fb363b9-1afd-4bca-dee9-1f5503e9b91d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i don't like chatting with you.</td>\n",
       "      <td>0</td>\n",
       "      <td>i do not like chatting with you .</td>\n",
       "      <td>[i, do, not, like, chatting, with, you, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you do not have friends, cause no body loves you.</td>\n",
       "      <td>1</td>\n",
       "      <td>you do not have friends , because no body love...</td>\n",
       "      <td>[you, do, not, have, friends, ,, because, no, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you don't deserve to exist.</td>\n",
       "      <td>1</td>\n",
       "      <td>you do not deserve to exist .</td>\n",
       "      <td>[you, do, not, deserve, to, exist, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no one cares about your opinion.</td>\n",
       "      <td>1</td>\n",
       "      <td>no one cares about your opinion .</td>\n",
       "      <td>[no, one, cares, about, your, opinion, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no one respect you.</td>\n",
       "      <td>1</td>\n",
       "      <td>no one respect you .</td>\n",
       "      <td>[no, one, respect, you, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label  \\\n",
       "0                    i don't like chatting with you.      0   \n",
       "1  you do not have friends, cause no body loves you.      1   \n",
       "2                        you don't deserve to exist.      1   \n",
       "3                   no one cares about your opinion.      1   \n",
       "4                                no one respect you.      1   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0                  i do not like chatting with you .   \n",
       "1  you do not have friends , because no body love...   \n",
       "2                      you do not deserve to exist .   \n",
       "3                  no one cares about your opinion .   \n",
       "4                               no one respect you .   \n",
       "\n",
       "                                           tokenized  \n",
       "0         [i, do, not, like, chatting, with, you, .]  \n",
       "1  [you, do, not, have, friends, ,, because, no, ...  \n",
       "2              [you, do, not, deserve, to, exist, .]  \n",
       "3          [no, one, cares, about, your, opinion, .]  \n",
       "4                         [no, one, respect, you, .]  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdh6w0c9gzki"
   },
   "source": [
    "###  Covert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "auI4cFbvqbCY",
    "outputId": "a95d5acd-c800-48de-ba08-403ef74a1bfc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i don't like chatting with you.</td>\n",
       "      <td>0</td>\n",
       "      <td>i do not like chatting with you .</td>\n",
       "      <td>[i, do, not, like, chatting, with, you, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you do not have friends, cause no body loves you.</td>\n",
       "      <td>1</td>\n",
       "      <td>you do not have friends , because no body love...</td>\n",
       "      <td>[you, do, not, have, friends, ,, because, no, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you don't deserve to exist.</td>\n",
       "      <td>1</td>\n",
       "      <td>you do not deserve to exist .</td>\n",
       "      <td>[you, do, not, deserve, to, exist, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no one cares about your opinion.</td>\n",
       "      <td>1</td>\n",
       "      <td>no one cares about your opinion .</td>\n",
       "      <td>[no, one, cares, about, your, opinion, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no one respect you.</td>\n",
       "      <td>1</td>\n",
       "      <td>no one respect you .</td>\n",
       "      <td>[no, one, respect, you, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label  \\\n",
       "0                    i don't like chatting with you.      0   \n",
       "1  you do not have friends, cause no body loves you.      1   \n",
       "2                        you don't deserve to exist.      1   \n",
       "3                   no one cares about your opinion.      1   \n",
       "4                                no one respect you.      1   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0                  i do not like chatting with you .   \n",
       "1  you do not have friends , because no body love...   \n",
       "2                      you do not deserve to exist .   \n",
       "3                  no one cares about your opinion .   \n",
       "4                               no one respect you .   \n",
       "\n",
       "                                           tokenized  \n",
       "0         [i, do, not, like, chatting, with, you, .]  \n",
       "1  [you, do, not, have, friends, ,, because, no, ...  \n",
       "2              [you, do, not, deserve, to, exist, .]  \n",
       "3          [no, one, cares, about, your, opinion, .]  \n",
       "4                         [no, one, respect, you, .]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: x.lower())\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvy5NH5CojyJ"
   },
   "source": [
    "### Remove any URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "uIHYHjOtrMIc"
   },
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    \"\"\"\n",
    "        Remove URLs from a sample string\n",
    "    \"\"\"\n",
    "\n",
    "    return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "m3d4iMiNrPn0"
   },
   "outputs": [],
   "source": [
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_URL(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFejSgwJos7I"
   },
   "source": [
    "### Remove HTML Tages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "w-DOPNF2r27r"
   },
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    \"\"\"\n",
    "        Remove the html in sample text\n",
    "    \"\"\"\n",
    "    html = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n",
    "    return re.sub(html, \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "sH3wQ3Emr5vP"
   },
   "outputs": [],
   "source": [
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_html(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TheQoNce00Tb"
   },
   "source": [
    "### Remove Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "IxpoqYt80t1x"
   },
   "outputs": [],
   "source": [
    "def Remove_numbers(text):\n",
    "\n",
    "    \"\"\"\n",
    "      Remove the numbers in text \n",
    "    \"\"\"\n",
    "    result = ''.join(i for i in text if not i.isdigit())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T9b62xX21XM_",
    "outputId": "c9fd0a0a-9a28-4418-91c7-2ba74e814f99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love you  times\n"
     ]
    }
   ],
   "source": [
    "s =  \"i love you 10000000000 times\"\n",
    "print(Remove_numbers(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "q9t5H2bn1SrZ"
   },
   "outputs": [],
   "source": [
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: Remove_numbers(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1Weuyiwo9NE"
   },
   "source": [
    "### Replace the Typos, slang, acronyms or informal abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "qQwTF4UWvFuq"
   },
   "outputs": [],
   "source": [
    "def informalAbbreviations_clean(text):\n",
    "        \"\"\"\n",
    "            Other manual text cleaning techniques\n",
    "        \"\"\"\n",
    "        # Typos, slang and other\n",
    "        sample_typos_slang = {\n",
    "                                \"w/e\": \"whatever\",\n",
    "                                \"usagov\": \"usa government\",\n",
    "                                \"recentlu\": \"recently\",\n",
    "                                \"ph0tos\": \"photos\",\n",
    "                                \"amirite\": \"am i right\",\n",
    "                                \"exp0sed\": \"exposed\",\n",
    "                                \"<3\": \"love\",\n",
    "                                \"luv\": \"love\",\n",
    "                                \"amageddon\": \"armageddon\",\n",
    "                                \"trfc\": \"traffic\",\n",
    "                                \"16yr\": \"16 year\"\n",
    "                                }\n",
    "\n",
    "        # Acronyms\n",
    "        sample_acronyms =  { \n",
    "                            \"mh370\": \"malaysia airlines flight 370\",\n",
    "                            \"okwx\": \"oklahoma city weather\",\n",
    "                            \"arwx\": \"arkansas weather\",    \n",
    "                            \"gawx\": \"georgia weather\",  \n",
    "                            \"scwx\": \"south carolina weather\",  \n",
    "                            \"cawx\": \"california weather\",\n",
    "                            \"tnwx\": \"tennessee weather\",\n",
    "                            \"azwx\": \"arizona weather\",  \n",
    "                            \"alwx\": \"alabama weather\",\n",
    "                            \"usnwsgov\": \"united states national weather service\",\n",
    "                            \"2mw\": \"tomorrow\"\n",
    "                            }\n",
    "\n",
    "        \n",
    "        # Some common abbreviations \n",
    "        sample_abbr = {\n",
    "                        \"$\" : \" dollar \",\n",
    "                        \"€\" : \" euro \",\n",
    "                        \"4ao\" : \"for adults only\",\n",
    "                        \"a.m\" : \"before midday\",\n",
    "                        \"a3\" : \"anytime anywhere anyplace\",\n",
    "                        \"aamof\" : \"as a matter of fact\",\n",
    "                        \"acct\" : \"account\",\n",
    "                        \"adih\" : \"another day in hell\",\n",
    "                        \"afaic\" : \"as far as i am concerned\",\n",
    "                        \"afaict\" : \"as far as i can tell\",\n",
    "                        \"afaik\" : \"as far as i know\",\n",
    "                        \"afair\" : \"as far as i remember\",\n",
    "                        \"afk\" : \"away from keyboard\",\n",
    "                        \"app\" : \"application\",\n",
    "                        \"approx\" : \"approximately\",\n",
    "                        \"apps\" : \"applications\",\n",
    "                        \"asap\" : \"as soon as possible\",\n",
    "                        \"asl\" : \"age, sex, location\",\n",
    "                        \"atk\" : \"at the keyboard\",\n",
    "                        \"ave.\" : \"avenue\",\n",
    "                        \"aymm\" : \"are you my mother\",\n",
    "                        \"ayor\" : \"at your own risk\", \n",
    "                        \"b&b\" : \"bed and breakfast\",\n",
    "                        \"b+b\" : \"bed and breakfast\",\n",
    "                        \"b.c\" : \"before christ\",\n",
    "                        \"b2b\" : \"business to business\",\n",
    "                        \"b2c\" : \"business to customer\",\n",
    "                        \"b4\" : \"before\",\n",
    "                        \"b4n\" : \"bye for now\",\n",
    "                        \"b@u\" : \"back at you\",\n",
    "                        \"bae\" : \"before anyone else\",\n",
    "                        \"bak\" : \"back at keyboard\",\n",
    "                        \"bbbg\" : \"bye bye be good\",\n",
    "                        \"bbc\" : \"british broadcasting corporation\",\n",
    "                        \"bbias\" : \"be back in a second\",\n",
    "                        \"bbl\" : \"be back later\",\n",
    "                        \"bbs\" : \"be back soon\",\n",
    "                        \"be4\" : \"before\",\n",
    "                        \"bfn\" : \"bye for now\",\n",
    "                        \"blvd\" : \"boulevard\",\n",
    "                        \"bout\" : \"about\",\n",
    "                        \"brb\" : \"be right back\",\n",
    "                        \"bros\" : \"brothers\",\n",
    "                        \"brt\" : \"be right there\",\n",
    "                        \"bsaaw\" : \"big smile and a wink\",\n",
    "                        \"btw\" : \"by the way\",\n",
    "                        \"bwl\" : \"bursting with laughter\",\n",
    "                        \"c/o\" : \"care of\",\n",
    "                        \"cet\" : \"central european time\",\n",
    "                        \"cf\" : \"compare\",\n",
    "                        \"cia\" : \"central intelligence agency\",\n",
    "                        \"csl\" : \"can not stop laughing\",\n",
    "                        \"cu\" : \"see you\",\n",
    "                        \"cul8r\" : \"see you later\",\n",
    "                        \"cv\" : \"curriculum vitae\",\n",
    "                        \"cwot\" : \"complete waste of time\",\n",
    "                        \"cya\" : \"see you\",\n",
    "                        \"cyt\" : \"see you tomorrow\",\n",
    "                        \"dae\" : \"does anyone else\",\n",
    "                        \"dbmib\" : \"do not bother me i am busy\",\n",
    "                        \"diy\" : \"do it yourself\",\n",
    "                        \"dm\" : \"direct message\",\n",
    "                        \"dwh\" : \"during work hours\",\n",
    "                        \"e123\" : \"easy as one two three\",\n",
    "                        \"eet\" : \"eastern european time\",\n",
    "                        \"eg\" : \"example\",\n",
    "                        \"embm\" : \"early morning business meeting\",\n",
    "                        \"encl\" : \"enclosed\",\n",
    "                        \"encl.\" : \"enclosed\",\n",
    "                        \"etc\" : \"and so on\",\n",
    "                        \"faq\" : \"frequently asked questions\",\n",
    "                        \"fawc\" : \"for anyone who cares\",\n",
    "                        \"fb\" : \"facebook\",\n",
    "                        \"fc\" : \"fingers crossed\",\n",
    "                        \"fig\" : \"figure\",\n",
    "                        \"fimh\" : \"forever in my heart\", \n",
    "                        \"ft.\" : \"feet\",\n",
    "                        \"ft\" : \"featuring\",\n",
    "                        \"ftl\" : \"for the loss\",\n",
    "                        \"ftw\" : \"for the win\",\n",
    "                        \"fwiw\" : \"for what it is worth\",\n",
    "                        \"fyi\" : \"for your information\",\n",
    "                        \"g9\" : \"genius\",\n",
    "                        \"gahoy\" : \"get a hold of yourself\",\n",
    "                        \"gal\" : \"get a life\",\n",
    "                        \"gcse\" : \"general certificate of secondary education\",\n",
    "                        \"gfn\" : \"gone for now\",\n",
    "                        \"gg\" : \"good game\",\n",
    "                        \"gl\" : \"good luck\",\n",
    "                        \"glhf\" : \"good luck have fun\",\n",
    "                        \"gmt\" : \"greenwich mean time\",\n",
    "                        \"gmta\" : \"great minds think alike\",\n",
    "                        \"gn\" : \"good night\",\n",
    "                        \"g.o.a.t\" : \"greatest of all time\",\n",
    "                        \"goat\" : \"greatest of all time\",\n",
    "                        \"goi\" : \"get over it\",\n",
    "                        \"gps\" : \"global positioning system\",\n",
    "                        \"gr8\" : \"great\",\n",
    "                        \"gratz\" : \"congratulations\",\n",
    "                        \"gyal\" : \"girl\",\n",
    "                        \"h&c\" : \"hot and cold\",\n",
    "                        \"hp\" : \"horsepower\",\n",
    "                        \"hr\" : \"hour\",\n",
    "                        \"hrh\" : \"his royal highness\",\n",
    "                        \"ht\" : \"height\",\n",
    "                        \"ibrb\" : \"i will be right back\",\n",
    "                        \"ic\" : \"i see\",\n",
    "                        \"icq\" : \"i seek you\",\n",
    "                        \"icymi\" : \"in case you missed it\",\n",
    "                        \"idc\" : \"i do not care\",\n",
    "                        \"idgadf\" : \"i do not give a damn fuck\",\n",
    "                        \"idgaf\" : \"i do not give a fuck\",\n",
    "                        \"idk\" : \"i do not know\",\n",
    "                        \"ie\" : \"that is\",\n",
    "                        \"i.e\" : \"that is\",\n",
    "                        \"ifyp\" : \"i feel your pain\",\n",
    "                        \"IG\" : \"instagram\",\n",
    "                        \"iirc\" : \"if i remember correctly\",\n",
    "                        \"ilu\" : \"i love you\",\n",
    "                        \"ily\" : \"i love you\",\n",
    "                        \"imho\" : \"in my humble opinion\",\n",
    "                        \"imo\" : \"in my opinion\",\n",
    "                        \"imu\" : \"i miss you\",\n",
    "                        \"iow\" : \"in other words\",\n",
    "                        \"irl\" : \"in real life\",\n",
    "                        \"j4f\" : \"just for fun\",\n",
    "                        \"jic\" : \"just in case\",\n",
    "                        \"jk\" : \"just kidding\",\n",
    "                        \"jsyk\" : \"just so you know\",\n",
    "                        \"l8r\" : \"later\",\n",
    "                        \"lb\" : \"pound\",\n",
    "                        \"lbs\" : \"pounds\",\n",
    "                        \"ldr\" : \"long distance relationship\",\n",
    "                        \"lmao\" : \"laugh my ass off\",\n",
    "                        \"lmfao\" : \"laugh my fucking ass off\",\n",
    "                        \"lol\" : \"laughing out loud\",\n",
    "                        \"ltd\" : \"limited\",\n",
    "                        \"ltns\" : \"long time no see\",\n",
    "                        \"m8\" : \"mate\",\n",
    "                        \"mf\" : \"motherfucker\",\n",
    "                        \"mfs\" : \"motherfuckers\",\n",
    "                        \"mfw\" : \"my face when\",\n",
    "                        \"mofo\" : \"motherfucker\",\n",
    "                        \"mph\" : \"miles per hour\",\n",
    "                        \"mr\" : \"mister\",\n",
    "                        \"mrw\" : \"my reaction when\",\n",
    "                        \"ms\" : \"miss\",\n",
    "                        \"mte\" : \"my thoughts exactly\",\n",
    "                        \"nagi\" : \"not a good idea\",\n",
    "                        \"nbc\" : \"national broadcasting company\",\n",
    "                        \"nbd\" : \"not big deal\",\n",
    "                        \"nfs\" : \"not for sale\",\n",
    "                        \"ngl\" : \"not going to lie\",\n",
    "                        \"nhs\" : \"national health service\",\n",
    "                        \"nrn\" : \"no reply necessary\",\n",
    "                        \"nsfl\" : \"not safe for life\",\n",
    "                        \"nsfw\" : \"not safe for work\",\n",
    "                        \"nth\" : \"nice to have\",\n",
    "                        \"nvr\" : \"never\",\n",
    "                        \"nyc\" : \"new york city\",\n",
    "                        \"oc\" : \"original content\",\n",
    "                        \"og\" : \"original\",\n",
    "                        \"ohp\" : \"overhead projector\",\n",
    "                        \"oic\" : \"oh i see\",\n",
    "                        \"omdb\" : \"over my dead body\",\n",
    "                        \"omg\" : \"oh my god\",\n",
    "                        \"omw\" : \"on my way\",\n",
    "                        \"p.a\" : \"per annum\",\n",
    "                        \"p.m\" : \"after midday\",\n",
    "                        \"pm\" : \"prime minister\",\n",
    "                        \"poc\" : \"people of color\",\n",
    "                        \"pov\" : \"point of view\",\n",
    "                        \"pp\" : \"pages\",\n",
    "                        \"ppl\" : \"people\",\n",
    "                        \"prw\" : \"parents are watching\",\n",
    "                        \"ps\" : \"postscript\",\n",
    "                        \"pt\" : \"point\",\n",
    "                        \"ptb\" : \"please text back\",\n",
    "                        \"pto\" : \"please turn over\",\n",
    "                        \"qpsa\" : \"what happens\", #\"que pasa\",\n",
    "                        \"ratchet\" : \"rude\",\n",
    "                        \"rbtl\" : \"read between the lines\",\n",
    "                        \"rlrt\" : \"real life retweet\", \n",
    "                        \"rofl\" : \"rolling on the floor laughing\",\n",
    "                        \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "                        \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "                        \"rt\" : \"retweet\",\n",
    "                        \"ruok\" : \"are you ok\",\n",
    "                        \"sfw\" : \"safe for work\",\n",
    "                        \"sk8\" : \"skate\",\n",
    "                        \"smh\" : \"shake my head\",\n",
    "                        \"sq\" : \"square\",\n",
    "                        \"srsly\" : \"seriously\", \n",
    "                        \"ssdd\" : \"same stuff different day\",\n",
    "                        \"tbh\" : \"to be honest\",\n",
    "                        \"tbs\" : \"tablespooful\",\n",
    "                        \"tbsp\" : \"tablespooful\",\n",
    "                        \"tfw\" : \"that feeling when\",\n",
    "                        \"thks\" : \"thank you\",\n",
    "                        \"tho\" : \"though\",\n",
    "                        \"thx\" : \"thank you\",\n",
    "                        \"tia\" : \"thanks in advance\",\n",
    "                        \"til\" : \"today i learned\",\n",
    "                        \"tl;dr\" : \"too long i did not read\",\n",
    "                        \"tldr\" : \"too long i did not read\",\n",
    "                        \"tmb\" : \"tweet me back\",\n",
    "                        \"tntl\" : \"trying not to laugh\",\n",
    "                        \"ttyl\" : \"talk to you later\",\n",
    "                        \"u\" : \"you\",\n",
    "                        \"u2\" : \"you too\",\n",
    "                        \"u4e\" : \"yours for ever\",\n",
    "                        \"utc\" : \"coordinated universal time\",\n",
    "                        \"w/\" : \"with\",\n",
    "                        \"w/o\" : \"without\",\n",
    "                        \"w8\" : \"wait\",\n",
    "                        \"wassup\" : \"what is up\",\n",
    "                        \"wb\" : \"welcome back\",\n",
    "                        \"wtf\" : \"what the fuck\",\n",
    "                        \"wtg\" : \"way to go\",\n",
    "                        \"wtpa\" : \"where the party at\",\n",
    "                        \"wuf\" : \"where are you from\",\n",
    "                        \"wuzup\" : \"what is up\",\n",
    "                        \"wywh\" : \"wish you were here\",\n",
    "                        \"yd\" : \"yard\",\n",
    "                        \"ygtr\" : \"you got that right\",\n",
    "                        \"ynk\" : \"you never know\",\n",
    "                        \"zzz\" : \"sleeping bored and tired\"\n",
    "                        }\n",
    "            \n",
    "        sample_typos_slang_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_typos_slang.keys()) + r')(?!\\w)')\n",
    "        sample_acronyms_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_acronyms.keys()) + r')(?!\\w)')\n",
    "        sample_abbr_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_abbr.keys()) + r')(?!\\w)')\n",
    "        \n",
    "        text = sample_typos_slang_pattern.sub(lambda x: sample_typos_slang[x.group()], text)\n",
    "        text = sample_acronyms_pattern.sub(lambda x: sample_acronyms[x.group()], text)\n",
    "        text = sample_abbr_pattern.sub(lambda x: sample_abbr[x.group()], text)\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "XinQSdMDz4eD"
   },
   "outputs": [],
   "source": [
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: informalAbbreviations_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "8I5fJ73U5e3_",
    "outputId": "5a7600c9-dca8-4572-d6d8-639825d6fcdc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i don't like chatting with you.</td>\n",
       "      <td>0</td>\n",
       "      <td>i do not like chatting with you .</td>\n",
       "      <td>[i, do, not, like, chatting, with, you, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you do not have friends, cause no body loves you.</td>\n",
       "      <td>1</td>\n",
       "      <td>you do not have friends , because no body love...</td>\n",
       "      <td>[you, do, not, have, friends, ,, because, no, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you don't deserve to exist.</td>\n",
       "      <td>1</td>\n",
       "      <td>you do not deserve to exist .</td>\n",
       "      <td>[you, do, not, deserve, to, exist, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no one cares about your opinion.</td>\n",
       "      <td>1</td>\n",
       "      <td>no one cares about your opinion .</td>\n",
       "      <td>[no, one, cares, about, your, opinion, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no one respect you.</td>\n",
       "      <td>1</td>\n",
       "      <td>no one respect you .</td>\n",
       "      <td>[no, one, respect, you, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label  \\\n",
       "0                    i don't like chatting with you.      0   \n",
       "1  you do not have friends, cause no body loves you.      1   \n",
       "2                        you don't deserve to exist.      1   \n",
       "3                   no one cares about your opinion.      1   \n",
       "4                                no one respect you.      1   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0                  i do not like chatting with you .   \n",
       "1  you do not have friends , because no body love...   \n",
       "2                      you do not deserve to exist .   \n",
       "3                  no one cares about your opinion .   \n",
       "4                               no one respect you .   \n",
       "\n",
       "                                           tokenized  \n",
       "0         [i, do, not, like, chatting, with, you, .]  \n",
       "1  [you, do, not, have, friends, ,, because, no, ...  \n",
       "2              [you, do, not, deserve, to, exist, .]  \n",
       "3          [no, one, cares, about, your, opinion, .]  \n",
       "4                         [no, one, respect, you, .]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQptCVO3sHbH"
   },
   "source": [
    "### Remove non ascii charcters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "hWVtKJQOr9gX"
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "\n",
    "    \"\"\"\n",
    "        Remove non-ASCII characters \n",
    "    \"\"\"\n",
    "\n",
    "    return re.sub(r'[^\\x00-\\x7f]',r'', text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "12PeMtcer9qS"
   },
   "outputs": [],
   "source": [
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_non_ascii(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDA5QDoKsN9c"
   },
   "source": [
    "### Remove any other special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "aK4HiF1ctyaM"
   },
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    \"\"\"\n",
    "        Remove special special characters, including symbols, emojis, and other graphic characters\n",
    "    \"\"\"\n",
    "    emoji_pattern = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "jJ-nIi9lt4q9"
   },
   "outputs": [],
   "source": [
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_special_characters(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling the negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def negate_sequence(text):\n",
    "#     negation = False\n",
    "#     delims = \"?.,!:;\"\n",
    "#     result = []\n",
    "#     words = text.split()\n",
    "#     prev = None\n",
    "#     pprev = None\n",
    "#     for word in words:\n",
    "#         stripped = word.strip(delims).lower()\n",
    "#         negated = \"not_\" + stripped if negation else stripped\n",
    "#         result.append(negated)\n",
    "\n",
    "#         if any(neg in word for neg in [\"not\", \"n't\", \"no\",'none','neither','nor','never']):\n",
    "#             negation = not negation\n",
    "\n",
    "#         if any(c in word for c in delims):\n",
    "#             negation = False\n",
    "\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.corpus import wordnet\n",
    "\n",
    "# def negation_function(x):   \n",
    "#     synonyms = []\n",
    "#     antonyms = []\n",
    "#     for syn in wordnet.synsets(x):\n",
    "#         for l in syn.lemmas():\n",
    "#             synonyms.append(l.name())\n",
    "#             if l.antonyms():\n",
    "#                 antonyms.append(l.antonyms()[0].name())\n",
    "               \n",
    "#     if(len(antonyms) != 0):\n",
    "#         return antonyms[0]\n",
    "#     else:\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_notWords(text):\n",
    "#     neg = [\"not\", \"n't\", \"no\",'none','neither','nor','never']\n",
    "#     if text in neg:\n",
    "#         return \"#\"\n",
    "#     else:\n",
    "#         sub_str =\"not_\"\n",
    "#         if (text.find(sub_str) != -1):\n",
    "#             text = text.replace('not_','')\n",
    "#             return negation_function(text) \n",
    "#         else:\n",
    "#             return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Handling_negation(text):\n",
    "#     split_negation = negate_sequence(text)\n",
    "#     full_text = []\n",
    "#     for i in range(len(split_negation)):\n",
    "#         full_text.append(find_notWords(split_negation[i]))\n",
    "#     full_text = list(filter(lambda x: x != \"#\", full_text))\n",
    "#     return full_text   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: Handling_negation(x))\n",
    "# train_df[\"text_clean\"] = [' '.join(map(str, l)) for l in train_df[\"text_clean\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i don't like chatting with you.</td>\n",
       "      <td>0</td>\n",
       "      <td>i do not like chatting with you .</td>\n",
       "      <td>[i, do, not, like, chatting, with, you, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you do not have friends, cause no body loves you.</td>\n",
       "      <td>1</td>\n",
       "      <td>you do not have friends , because no body love...</td>\n",
       "      <td>[you, do, not, have, friends, ,, because, no, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you don't deserve to exist.</td>\n",
       "      <td>1</td>\n",
       "      <td>you do not deserve to exist .</td>\n",
       "      <td>[you, do, not, deserve, to, exist, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no one cares about your opinion.</td>\n",
       "      <td>1</td>\n",
       "      <td>no one cares about your opinion .</td>\n",
       "      <td>[no, one, cares, about, your, opinion, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no one respect you.</td>\n",
       "      <td>1</td>\n",
       "      <td>no one respect you .</td>\n",
       "      <td>[no, one, respect, you, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label  \\\n",
       "0                    i don't like chatting with you.      0   \n",
       "1  you do not have friends, cause no body loves you.      1   \n",
       "2                        you don't deserve to exist.      1   \n",
       "3                   no one cares about your opinion.      1   \n",
       "4                                no one respect you.      1   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0                  i do not like chatting with you .   \n",
       "1  you do not have friends , because no body love...   \n",
       "2                      you do not deserve to exist .   \n",
       "3                  no one cares about your opinion .   \n",
       "4                               no one respect you .   \n",
       "\n",
       "                                           tokenized  \n",
       "0         [i, do, not, like, chatting, with, you, .]  \n",
       "1  [you, do, not, have, friends, ,, because, no, ...  \n",
       "2              [you, do, not, deserve, to, exist, .]  \n",
       "3          [no, one, cares, about, your, opinion, .]  \n",
       "4                         [no, one, respect, you, .]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdFa7rJ4scmX"
   },
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Cd5lpBLMt7gF"
   },
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    \"\"\"\n",
    "        Remove the punctuation\n",
    "    \"\"\"\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Q3ba7b_qvC_y"
   },
   "outputs": [],
   "source": [
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkITUuQQCBGQ"
   },
   "source": [
    "### Remove empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "uueNAwOTCJ-X"
   },
   "outputs": [],
   "source": [
    "train_df[\"text_clean\"].replace('', np.nan, inplace=True)\n",
    "train_df.dropna(subset=[\"text_clean\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfOjDB_lhZRL"
   },
   "source": [
    "\n",
    "# Text Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Your mindset is not nice as you think it is.</td>\n",
       "      <td>1</td>\n",
       "      <td>your mindset is not nice as you think it is</td>\n",
       "      <td>[Your, mindset, is, not, nice, as, you, think,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>you are not ugly.</td>\n",
       "      <td>0</td>\n",
       "      <td>you are not ugly</td>\n",
       "      <td>[you, are, not, ugly, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>you are not stuip.</td>\n",
       "      <td>1</td>\n",
       "      <td>you are not stuip</td>\n",
       "      <td>[you, are, not, stuip, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>She turned to be not ugly as we thought she</td>\n",
       "      <td>1</td>\n",
       "      <td>she turned to be not ugly as we thought she</td>\n",
       "      <td>[She, turned, to, be, not, ugly, as, we, thoug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>We did not love her before but we do now.</td>\n",
       "      <td>1</td>\n",
       "      <td>we did not love her before but we do now</td>\n",
       "      <td>[We, did, not, love, her, before, but, we, do,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweet  label  \\\n",
       "10  Your mindset is not nice as you think it is.      1   \n",
       "11                             you are not ugly.      0   \n",
       "12                            you are not stuip.      1   \n",
       "13   She turned to be not ugly as we thought she      1   \n",
       "14     We did not love her before but we do now.      1   \n",
       "\n",
       "                                      text_clean  \\\n",
       "10  your mindset is not nice as you think it is    \n",
       "11                             you are not ugly    \n",
       "12                            you are not stuip    \n",
       "13   she turned to be not ugly as we thought she   \n",
       "14     we did not love her before but we do now    \n",
       "\n",
       "                                            tokenized  \n",
       "10  [Your, mindset, is, not, nice, as, you, think,...  \n",
       "11                           [you, are, not, ugly, .]  \n",
       "12                          [you, are, not, stuip, .]  \n",
       "13  [She, turned, to, be, not, ugly, as, we, thoug...  \n",
       "14  [We, did, not, love, her, before, but, we, do,...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bAT3-7Xtw7_"
   },
   "source": [
    "### **Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "NSlKvw5IhYAp",
    "outputId": "6c21968f-c814-4747-8075-a3f56a71c0fd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i don't like chatting with you.</td>\n",
       "      <td>0</td>\n",
       "      <td>i do not like chatting with you</td>\n",
       "      <td>[i, do, not, like, chatting, with, you]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you do not have friends, cause no body loves you.</td>\n",
       "      <td>1</td>\n",
       "      <td>you do not have friends  because no body loves...</td>\n",
       "      <td>[you, do, not, have, friends, because, no, bod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you don't deserve to exist.</td>\n",
       "      <td>1</td>\n",
       "      <td>you do not deserve to exist</td>\n",
       "      <td>[you, do, not, deserve, to, exist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no one cares about your opinion.</td>\n",
       "      <td>1</td>\n",
       "      <td>no one cares about your opinion</td>\n",
       "      <td>[no, one, cares, about, your, opinion]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no one respect you.</td>\n",
       "      <td>1</td>\n",
       "      <td>no one respect you</td>\n",
       "      <td>[no, one, respect, you]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label  \\\n",
       "0                    i don't like chatting with you.      0   \n",
       "1  you do not have friends, cause no body loves you.      1   \n",
       "2                        you don't deserve to exist.      1   \n",
       "3                   no one cares about your opinion.      1   \n",
       "4                                no one respect you.      1   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0                   i do not like chatting with you    \n",
       "1  you do not have friends  because no body loves...   \n",
       "2                       you do not deserve to exist    \n",
       "3                   no one cares about your opinion    \n",
       "4                                no one respect you    \n",
       "\n",
       "                                           tokenized  \n",
       "0            [i, do, not, like, chatting, with, you]  \n",
       "1  [you, do, not, have, friends, because, no, bod...  \n",
       "2                 [you, do, not, deserve, to, exist]  \n",
       "3             [no, one, cares, about, your, opinion]  \n",
       "4                            [no, one, respect, you]  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "train_df[\"tokenized\"] = train_df[\"text_clean\"].apply(word_tokenize)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHM4_fkLhOE9"
   },
   "source": [
    "\n",
    "### **Remove stop word without the negation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My_stopWordList=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S57840OahNVx",
    "outputId": "81020f56-f84e-44a0-be7b-a86a44bfe593"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1108)>\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "train_df['stopwords_removed'] = train_df[\"tokenized\"].apply(lambda x: [word for word in x if word not in stop ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEAsIAOmjplO"
   },
   "source": [
    "### **Part of Speech Tagging (POS Tagging):**\n",
    "\n",
    "Part of speech tagging (POS tagging) distinguishes the part of speech (noun, verb, adjective, and etc.) of each word in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X93d9jNBjpKw",
    "outputId": "a5946792-8431-4ccb-9711-01016e8231f9"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import brown\n",
    "\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \n",
    "               \"V\":wordnet.VERB, \n",
    "               \"J\":wordnet.ADJ, \n",
    "               \"R\":wordnet.ADV\n",
    "              }\n",
    "    \n",
    "train_sents = brown.tagged_sents(categories='news')\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "\n",
    "def pos_tag_wordnet(text, pos_tag_type=\"pos_tag\"):\n",
    "    \"\"\"\n",
    "        Create pos_tag with wordnet format\n",
    "    \"\"\"\n",
    "    pos_tagged_text = t2.tag(text)\n",
    "    \n",
    "    # map the pos tagging output with wordnet output \n",
    "    pos_tagged_text = [(word, wordnet_map.get(pos_tag[0])) if pos_tag[0] in wordnet_map.keys() else (word, wordnet.NOUN) for (word, pos_tag) in pos_tagged_text ]\n",
    "    return pos_tagged_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "PsxJo-ZMjzed"
   },
   "outputs": [],
   "source": [
    "train_df['pos_tag'] = train_df['stopwords_removed'].apply(lambda x: pos_tag_wordnet(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovWC2bDEj8BS"
   },
   "source": [
    "### **Lemmatization:**\n",
    "\n",
    "Lemmatization is the task of determining that two words have the same root, despite their surface differences. The words am, are, and is have the shared lemma be; the words dinner and dinners both have the lemma dinner. \n",
    "Lemmatizing each of these forms to the same lemma will let us ﬁnd all mentions of words in Russian like Moscow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "21FBhMGsS0t1"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    \"\"\"\n",
    "        Lemmatize the tokenized words\n",
    "    \"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma = [lemmatizer.lemmatize(word, tag) for word, tag in text]\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import WordNetLemmatizer as form_replacer\n",
    "# lemmatized_irr2 = list(set([form_replacer().lemmatize(x) for x in ['isis','one']]))\n",
    "# print(lemmatized_irr2 )\n",
    "# #apply(lambda x: [lemmatizer.lemmatize(word) for word in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "Y-AQijDVkNRs"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "train_df['lemmatize_word_w_pos'] = train_df['pos_tag'].apply(lambda x: lemmatize_word(x))\n",
    "train_df['lemmatize_text'] = [' '.join(map(str, l)) for l in train_df['lemmatize_word_w_pos']] # join back to text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double check that there is not any empty row after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['lemmatize_text'].replace('', np.nan, inplace=True)\n",
    "train_df.dropna(subset=['lemmatize_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UtI18Fft6pA"
   },
   "source": [
    "### **Saving Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "oPgGeR4j9NKz"
   },
   "outputs": [],
   "source": [
    "train_df['lemmatize_text'].to_csv('tryyyyyyyyyyyyyyyy.csv')# choose the name to be saved with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textblob import TextBlob\n",
    "\n",
    "# # Preparing an input sentence\n",
    "# sentence = 'i do dislike chatting with you'\n",
    "\n",
    "# analysisPol = TextBlob(sentence).polarity\n",
    "\n",
    "\n",
    "# print(analysisPol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GP Model 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
