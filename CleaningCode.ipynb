{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "POQpUq40q8k5",
    "outputId": "261c9190-fd57-432b-82ff-c5568e033a11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.0.1-cp38-cp38-win_amd64.whl (23.9 MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\lenovo\\.conda\\envs\\gpu\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\lenovo\\.conda\\envs\\gpu\\lib\\site-packages (from gensim) (1.19.5)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.0.0-py3-none-any.whl (56 kB)\n",
      "Collecting Cython==0.29.21\n",
      "  Downloading Cython-0.29.21-cp38-cp38-win_amd64.whl (1.7 MB)\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.22\n",
      "    Uninstalling Cython-0.29.22:\n",
      "      Successfully uninstalled Cython-0.29.22\n",
      "Successfully installed Cython-0.29.21 gensim-4.0.1 smart-open-5.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\.conda\\envs\\gpu\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras\n",
    "import io \n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "import emoji\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import regexp_tokenize   # module for tokenizing strings\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import nltk \n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.utils import layer_utils\n",
    "from keras import backend as K\n",
    "!pip install gensim\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "zKq9SfCjqAkT",
    "outputId": "84e19602-4fb0-4648-fd3e-05d6bd4b19d8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>count_word</th>\n",
       "      <th>count_unique_word</th>\n",
       "      <th>count_letters</th>\n",
       "      <th>count_punctuations</th>\n",
       "      <th>count_stopwords</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>word_unique_percent</th>\n",
       "      <th>punct_percent</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>you_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Get fucking real dude.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.020690</td>\n",
       "      <td>0.012666</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>She is as dirty as they come  and that crook ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.158621</td>\n",
       "      <td>0.074266</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>4.080000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>why did you fuck it up. I could do it all day...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084795</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.074266</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>0.088542</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>86.666667</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.06250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dude they dont finish enclosing the fucking s...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058480</td>\n",
       "      <td>0.131034</td>\n",
       "      <td>0.071963</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>95.238095</td>\n",
       "      <td>19.047619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>WTF are you talking about Men? No men thats n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040936</td>\n",
       "      <td>0.096552</td>\n",
       "      <td>0.042602</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.036458</td>\n",
       "      <td>3.933333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.03125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  label                                              tweet  \\\n",
       "0           0      1                             Get fucking real dude.   \n",
       "1           1      1   She is as dirty as they come  and that crook ...   \n",
       "2           2      1   why did you fuck it up. I could do it all day...   \n",
       "3           3      1   Dude they dont finish enclosing the fucking s...   \n",
       "4           4      1   WTF are you talking about Men? No men thats n...   \n",
       "\n",
       "   count_sent  count_word  count_unique_word  count_letters  \\\n",
       "0         0.0    0.008772           0.020690       0.012666   \n",
       "1         0.0    0.070175           0.158621       0.074266   \n",
       "2         0.0    0.084795           0.172414       0.074266   \n",
       "3         0.0    0.058480           0.131034       0.071963   \n",
       "4         0.0    0.040936           0.096552       0.042602   \n",
       "\n",
       "   count_punctuations  count_stopwords  mean_word_len  word_unique_percent  \\\n",
       "0            0.005208         0.000000       4.750000           100.000000   \n",
       "1            0.026042         0.062500       4.080000            96.000000   \n",
       "2            0.026042         0.088542       3.333333            86.666667   \n",
       "3            0.020833         0.026042       5.000000            95.238095   \n",
       "4            0.015625         0.036458       3.933333           100.000000   \n",
       "\n",
       "   punct_percent  num_exclamation_marks  num_question_marks  you_count  \n",
       "0      25.000000                    0.0            0.000000    0.00000  \n",
       "1      20.000000                    0.0            0.000000    0.00000  \n",
       "2      16.666667                    0.0            0.000000    0.06250  \n",
       "3      19.047619                    0.0            0.014706    0.00000  \n",
       "4      20.000000                    0.0            0.014706    0.03125  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(r'C:\\Users\\Lenovo\\Desktop\\GP Code\\kaggel codes\\30k_featuers.csv')# change path to csv file\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgvB2kbWqWKh"
   },
   "source": [
    "# Clean Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7kn7vX4ox-y"
   },
   "source": [
    "###Remove @ username in Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "6hOGd94tsL8n"
   },
   "outputs": [],
   "source": [
    "def remove_at_user(text):\n",
    "\n",
    "    \"\"\"\n",
    "      Remove @username from tweets\n",
    "    \"\"\"\n",
    "#     print(text)\n",
    "    return re.sub(\"@[A-Za-z0-9]+\",\"\",str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "hjhj4Vv5tSzj"
   },
   "outputs": [],
   "source": [
    "train_df[\"text_clean\"] = train_df[\"tweet\"].apply(lambda x: remove_at_user(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b1XThQboDLi"
   },
   "source": [
    "###  Contractions \n",
    "\n",
    "We use the contractions package to expand the contraction in English such as we'll -> we will or we shouldn't've -> we should not have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SdIiCz30q_c0",
    "outputId": "17deb78e-c79a-49bd-e969-4b448168d56f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\lenovo\\.conda\\envs\\gpu\\lib\\site-packages (0.0.48)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\lenovo\\.conda\\envs\\gpu\\lib\\site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: anyascii in c:\\users\\lenovo\\.conda\\envs\\gpu\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.2.0)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\lenovo\\.conda\\envs\\gpu\\lib\\site-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "Kx5ic9egrlK-"
   },
   "outputs": [],
   "source": [
    "import contractions\n",
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: contractions.fix(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHdZgf8Sgoi5"
   },
   "source": [
    "### Repeat the captial words \n",
    "Here we repeat the Captial words to confirm the meaning of this word as the user try to focus on it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "l8zIkM2igy5i",
    "outputId": "b0874d5c-fea1-4d53-f574-526e44eb9026"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\.conda\\envs\\gpu\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\.conda\\envs\\gpu\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\lenovo\\.conda\\envs\\gpu\\lib\\site-packages (from nltk) (2021.3.17)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\.conda\\envs\\gpu\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\.conda\\envs\\gpu\\lib\\site-packages (from nltk) (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1108)>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>count_word</th>\n",
       "      <th>count_unique_word</th>\n",
       "      <th>count_letters</th>\n",
       "      <th>count_punctuations</th>\n",
       "      <th>count_stopwords</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>word_unique_percent</th>\n",
       "      <th>punct_percent</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>you_count</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Get fucking real dude.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.020690</td>\n",
       "      <td>0.012666</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>Get fucking real dude.</td>\n",
       "      <td>[Get, fucking, real, dude, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>She is as dirty as they come  and that crook ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.158621</td>\n",
       "      <td>0.074266</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>4.080000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>She is as dirty as they come  and that crook ...</td>\n",
       "      <td>[She, is, as, dirty, as, they, come, and, that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>why did you fuck it up. I could do it all day...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084795</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.074266</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>0.088542</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>86.666667</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.06250</td>\n",
       "      <td>why did you fuck it up. I could do it all day...</td>\n",
       "      <td>[why, did, you, fuck, it, up, ., I, could, do,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dude they dont finish enclosing the fucking s...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058480</td>\n",
       "      <td>0.131034</td>\n",
       "      <td>0.071963</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>95.238095</td>\n",
       "      <td>19.047619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>Dude they do not finish enclosing the fucking...</td>\n",
       "      <td>[Dude, they, do, not, finish, enclosing, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>WTF are you talking about Men? No men thats n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040936</td>\n",
       "      <td>0.096552</td>\n",
       "      <td>0.042602</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.036458</td>\n",
       "      <td>3.933333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>WTF are you talking about Men? No men that is...</td>\n",
       "      <td>[WTF, are, you, talking, about, Men, ?, No, me...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  label                                              tweet  \\\n",
       "0           0      1                             Get fucking real dude.   \n",
       "1           1      1   She is as dirty as they come  and that crook ...   \n",
       "2           2      1   why did you fuck it up. I could do it all day...   \n",
       "3           3      1   Dude they dont finish enclosing the fucking s...   \n",
       "4           4      1   WTF are you talking about Men? No men thats n...   \n",
       "\n",
       "   count_sent  count_word  count_unique_word  count_letters  \\\n",
       "0         0.0    0.008772           0.020690       0.012666   \n",
       "1         0.0    0.070175           0.158621       0.074266   \n",
       "2         0.0    0.084795           0.172414       0.074266   \n",
       "3         0.0    0.058480           0.131034       0.071963   \n",
       "4         0.0    0.040936           0.096552       0.042602   \n",
       "\n",
       "   count_punctuations  count_stopwords  mean_word_len  word_unique_percent  \\\n",
       "0            0.005208         0.000000       4.750000           100.000000   \n",
       "1            0.026042         0.062500       4.080000            96.000000   \n",
       "2            0.026042         0.088542       3.333333            86.666667   \n",
       "3            0.020833         0.026042       5.000000            95.238095   \n",
       "4            0.015625         0.036458       3.933333           100.000000   \n",
       "\n",
       "   punct_percent  num_exclamation_marks  num_question_marks  you_count  \\\n",
       "0      25.000000                    0.0            0.000000    0.00000   \n",
       "1      20.000000                    0.0            0.000000    0.00000   \n",
       "2      16.666667                    0.0            0.000000    0.06250   \n",
       "3      19.047619                    0.0            0.014706    0.00000   \n",
       "4      20.000000                    0.0            0.014706    0.03125   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0                             Get fucking real dude.   \n",
       "1   She is as dirty as they come  and that crook ...   \n",
       "2   why did you fuck it up. I could do it all day...   \n",
       "3   Dude they do not finish enclosing the fucking...   \n",
       "4   WTF are you talking about Men? No men that is...   \n",
       "\n",
       "                                           tokenized  \n",
       "0                      [Get, fucking, real, dude, .]  \n",
       "1  [She, is, as, dirty, as, they, come, and, that...  \n",
       "2  [why, did, you, fuck, it, up, ., I, could, do,...  \n",
       "3  [Dude, they, do, not, finish, enclosing, the, ...  \n",
       "4  [WTF, are, you, talking, about, Men, ?, No, me...  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "train_df['tokenized'] = train_df[\"text_clean\"].apply(word_tokenize)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "f7BqLRrIIaoa"
   },
   "outputs": [],
   "source": [
    "train_df['text_clean']=train_df['tokenized'].apply(lambda x:  [word  if word.isupper() == False else word+\" \"+word  for word in x]) \n",
    "train_df['text_clean'] = [' '.join(map(str, l)) for l in train_df['text_clean']] # join back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "XSid1k5Tl267",
    "outputId": "5fb363b9-1afd-4bca-dee9-1f5503e9b91d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>count_word</th>\n",
       "      <th>count_unique_word</th>\n",
       "      <th>count_letters</th>\n",
       "      <th>count_punctuations</th>\n",
       "      <th>count_stopwords</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>word_unique_percent</th>\n",
       "      <th>punct_percent</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>you_count</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Get fucking real dude.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.020690</td>\n",
       "      <td>0.012666</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>Get fucking real dude .</td>\n",
       "      <td>[Get, fucking, real, dude, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>She is as dirty as they come  and that crook ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.158621</td>\n",
       "      <td>0.074266</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>4.080000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>She is as dirty as they come and that crook Re...</td>\n",
       "      <td>[She, is, as, dirty, as, they, come, and, that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>why did you fuck it up. I could do it all day...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084795</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.074266</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>0.088542</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>86.666667</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.06250</td>\n",
       "      <td>why did you fuck it up . I I could do it all d...</td>\n",
       "      <td>[why, did, you, fuck, it, up, ., I, could, do,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dude they dont finish enclosing the fucking s...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058480</td>\n",
       "      <td>0.131034</td>\n",
       "      <td>0.071963</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>95.238095</td>\n",
       "      <td>19.047619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>Dude they do not finish enclosing the fucking ...</td>\n",
       "      <td>[Dude, they, do, not, finish, enclosing, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>WTF are you talking about Men? No men thats n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040936</td>\n",
       "      <td>0.096552</td>\n",
       "      <td>0.042602</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.036458</td>\n",
       "      <td>3.933333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>WTF WTF are you talking about Men ? No men tha...</td>\n",
       "      <td>[WTF, are, you, talking, about, Men, ?, No, me...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  label                                              tweet  \\\n",
       "0           0      1                             Get fucking real dude.   \n",
       "1           1      1   She is as dirty as they come  and that crook ...   \n",
       "2           2      1   why did you fuck it up. I could do it all day...   \n",
       "3           3      1   Dude they dont finish enclosing the fucking s...   \n",
       "4           4      1   WTF are you talking about Men? No men thats n...   \n",
       "\n",
       "   count_sent  count_word  count_unique_word  count_letters  \\\n",
       "0         0.0    0.008772           0.020690       0.012666   \n",
       "1         0.0    0.070175           0.158621       0.074266   \n",
       "2         0.0    0.084795           0.172414       0.074266   \n",
       "3         0.0    0.058480           0.131034       0.071963   \n",
       "4         0.0    0.040936           0.096552       0.042602   \n",
       "\n",
       "   count_punctuations  count_stopwords  mean_word_len  word_unique_percent  \\\n",
       "0            0.005208         0.000000       4.750000           100.000000   \n",
       "1            0.026042         0.062500       4.080000            96.000000   \n",
       "2            0.026042         0.088542       3.333333            86.666667   \n",
       "3            0.020833         0.026042       5.000000            95.238095   \n",
       "4            0.015625         0.036458       3.933333           100.000000   \n",
       "\n",
       "   punct_percent  num_exclamation_marks  num_question_marks  you_count  \\\n",
       "0      25.000000                    0.0            0.000000    0.00000   \n",
       "1      20.000000                    0.0            0.000000    0.00000   \n",
       "2      16.666667                    0.0            0.000000    0.06250   \n",
       "3      19.047619                    0.0            0.014706    0.00000   \n",
       "4      20.000000                    0.0            0.014706    0.03125   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0                            Get fucking real dude .   \n",
       "1  She is as dirty as they come and that crook Re...   \n",
       "2  why did you fuck it up . I I could do it all d...   \n",
       "3  Dude they do not finish enclosing the fucking ...   \n",
       "4  WTF WTF are you talking about Men ? No men tha...   \n",
       "\n",
       "                                           tokenized  \n",
       "0                      [Get, fucking, real, dude, .]  \n",
       "1  [She, is, as, dirty, as, they, come, and, that...  \n",
       "2  [why, did, you, fuck, it, up, ., I, could, do,...  \n",
       "3  [Dude, they, do, not, finish, enclosing, the, ...  \n",
       "4  [WTF, are, you, talking, about, Men, ?, No, me...  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdh6w0c9gzki"
   },
   "source": [
    "###  Covert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "auI4cFbvqbCY",
    "outputId": "a95d5acd-c800-48de-ba08-403ef74a1bfc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>count_word</th>\n",
       "      <th>count_unique_word</th>\n",
       "      <th>count_letters</th>\n",
       "      <th>count_punctuations</th>\n",
       "      <th>count_stopwords</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>word_unique_percent</th>\n",
       "      <th>punct_percent</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>you_count</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Get fucking real dude.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.020690</td>\n",
       "      <td>0.012666</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>get fucking real dude .</td>\n",
       "      <td>[Get, fucking, real, dude, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>She is as dirty as they come  and that crook ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.158621</td>\n",
       "      <td>0.074266</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>4.080000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>she is as dirty as they come and that crook re...</td>\n",
       "      <td>[She, is, as, dirty, as, they, come, and, that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>why did you fuck it up. I could do it all day...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084795</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.074266</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>0.088542</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>86.666667</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.06250</td>\n",
       "      <td>why did you fuck it up . i i could do it all d...</td>\n",
       "      <td>[why, did, you, fuck, it, up, ., I, could, do,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dude they dont finish enclosing the fucking s...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058480</td>\n",
       "      <td>0.131034</td>\n",
       "      <td>0.071963</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>95.238095</td>\n",
       "      <td>19.047619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>dude they do not finish enclosing the fucking ...</td>\n",
       "      <td>[Dude, they, do, not, finish, enclosing, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>WTF are you talking about Men? No men thats n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040936</td>\n",
       "      <td>0.096552</td>\n",
       "      <td>0.042602</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.036458</td>\n",
       "      <td>3.933333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>wtf wtf are you talking about men ? no men tha...</td>\n",
       "      <td>[WTF, are, you, talking, about, Men, ?, No, me...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  label                                              tweet  \\\n",
       "0           0      1                             Get fucking real dude.   \n",
       "1           1      1   She is as dirty as they come  and that crook ...   \n",
       "2           2      1   why did you fuck it up. I could do it all day...   \n",
       "3           3      1   Dude they dont finish enclosing the fucking s...   \n",
       "4           4      1   WTF are you talking about Men? No men thats n...   \n",
       "\n",
       "   count_sent  count_word  count_unique_word  count_letters  \\\n",
       "0         0.0    0.008772           0.020690       0.012666   \n",
       "1         0.0    0.070175           0.158621       0.074266   \n",
       "2         0.0    0.084795           0.172414       0.074266   \n",
       "3         0.0    0.058480           0.131034       0.071963   \n",
       "4         0.0    0.040936           0.096552       0.042602   \n",
       "\n",
       "   count_punctuations  count_stopwords  mean_word_len  word_unique_percent  \\\n",
       "0            0.005208         0.000000       4.750000           100.000000   \n",
       "1            0.026042         0.062500       4.080000            96.000000   \n",
       "2            0.026042         0.088542       3.333333            86.666667   \n",
       "3            0.020833         0.026042       5.000000            95.238095   \n",
       "4            0.015625         0.036458       3.933333           100.000000   \n",
       "\n",
       "   punct_percent  num_exclamation_marks  num_question_marks  you_count  \\\n",
       "0      25.000000                    0.0            0.000000    0.00000   \n",
       "1      20.000000                    0.0            0.000000    0.00000   \n",
       "2      16.666667                    0.0            0.000000    0.06250   \n",
       "3      19.047619                    0.0            0.014706    0.00000   \n",
       "4      20.000000                    0.0            0.014706    0.03125   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0                            get fucking real dude .   \n",
       "1  she is as dirty as they come and that crook re...   \n",
       "2  why did you fuck it up . i i could do it all d...   \n",
       "3  dude they do not finish enclosing the fucking ...   \n",
       "4  wtf wtf are you talking about men ? no men tha...   \n",
       "\n",
       "                                           tokenized  \n",
       "0                      [Get, fucking, real, dude, .]  \n",
       "1  [She, is, as, dirty, as, they, come, and, that...  \n",
       "2  [why, did, you, fuck, it, up, ., I, could, do,...  \n",
       "3  [Dude, they, do, not, finish, enclosing, the, ...  \n",
       "4  [WTF, are, you, talking, about, Men, ?, No, me...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: x.lower())\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvy5NH5CojyJ"
   },
   "source": [
    "### Remove any URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "uIHYHjOtrMIc"
   },
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    \"\"\"\n",
    "        Remove URLs from a sample string\n",
    "    \"\"\"\n",
    "\n",
    "    return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "m3d4iMiNrPn0"
   },
   "outputs": [],
   "source": [
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_URL(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFejSgwJos7I"
   },
   "source": [
    "### Remove HTML Tages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "w-DOPNF2r27r"
   },
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    \"\"\"\n",
    "        Remove the html in sample text\n",
    "    \"\"\"\n",
    "    html = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n",
    "    return re.sub(html, \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "sH3wQ3Emr5vP"
   },
   "outputs": [],
   "source": [
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_html(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TheQoNce00Tb"
   },
   "source": [
    "### Remove Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "IxpoqYt80t1x"
   },
   "outputs": [],
   "source": [
    "def Remove_numbers(text):\n",
    "\n",
    "    \"\"\"\n",
    "      Remove the numbers in text \n",
    "    \"\"\"\n",
    "    result = ''.join(i for i in text if not i.isdigit())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T9b62xX21XM_",
    "outputId": "c9fd0a0a-9a28-4418-91c7-2ba74e814f99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love you  times\n"
     ]
    }
   ],
   "source": [
    "s =  \"i love you 10000000000 times\"\n",
    "print(Remove_numbers(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "q9t5H2bn1SrZ"
   },
   "outputs": [],
   "source": [
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: Remove_numbers(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1Weuyiwo9NE"
   },
   "source": [
    "### Replace the Typos, slang, acronyms or informal abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "qQwTF4UWvFuq"
   },
   "outputs": [],
   "source": [
    "def informalAbbreviations_clean(text):\n",
    "        \"\"\"\n",
    "            Other manual text cleaning techniques\n",
    "        \"\"\"\n",
    "        # Typos, slang and other\n",
    "        sample_typos_slang = {\n",
    "                                \"w/e\": \"whatever\",\n",
    "                                \"usagov\": \"usa government\",\n",
    "                                \"recentlu\": \"recently\",\n",
    "                                \"ph0tos\": \"photos\",\n",
    "                                \"amirite\": \"am i right\",\n",
    "                                \"exp0sed\": \"exposed\",\n",
    "                                \"<3\": \"love\",\n",
    "                                \"luv\": \"love\",\n",
    "                                \"amageddon\": \"armageddon\",\n",
    "                                \"trfc\": \"traffic\",\n",
    "                                \"16yr\": \"16 year\"\n",
    "                                }\n",
    "\n",
    "        # Acronyms\n",
    "        sample_acronyms =  { \n",
    "                            \"mh370\": \"malaysia airlines flight 370\",\n",
    "                            \"okwx\": \"oklahoma city weather\",\n",
    "                            \"arwx\": \"arkansas weather\",    \n",
    "                            \"gawx\": \"georgia weather\",  \n",
    "                            \"scwx\": \"south carolina weather\",  \n",
    "                            \"cawx\": \"california weather\",\n",
    "                            \"tnwx\": \"tennessee weather\",\n",
    "                            \"azwx\": \"arizona weather\",  \n",
    "                            \"alwx\": \"alabama weather\",\n",
    "                            \"usnwsgov\": \"united states national weather service\",\n",
    "                            \"2mw\": \"tomorrow\"\n",
    "                            }\n",
    "\n",
    "        \n",
    "        # Some common abbreviations \n",
    "        sample_abbr = {\n",
    "                        \"$\" : \" dollar \",\n",
    "                        \"â‚¬\" : \" euro \",\n",
    "                        \"4ao\" : \"for adults only\",\n",
    "                        \"a.m\" : \"before midday\",\n",
    "                        \"a3\" : \"anytime anywhere anyplace\",\n",
    "                        \"aamof\" : \"as a matter of fact\",\n",
    "                        \"acct\" : \"account\",\n",
    "                        \"adih\" : \"another day in hell\",\n",
    "                        \"afaic\" : \"as far as i am concerned\",\n",
    "                        \"afaict\" : \"as far as i can tell\",\n",
    "                        \"afaik\" : \"as far as i know\",\n",
    "                        \"afair\" : \"as far as i remember\",\n",
    "                        \"afk\" : \"away from keyboard\",\n",
    "                        \"app\" : \"application\",\n",
    "                        \"approx\" : \"approximately\",\n",
    "                        \"apps\" : \"applications\",\n",
    "                        \"asap\" : \"as soon as possible\",\n",
    "                        \"asl\" : \"age, sex, location\",\n",
    "                        \"atk\" : \"at the keyboard\",\n",
    "                        \"ave.\" : \"avenue\",\n",
    "                        \"aymm\" : \"are you my mother\",\n",
    "                        \"ayor\" : \"at your own risk\", \n",
    "                        \"b&b\" : \"bed and breakfast\",\n",
    "                        \"b+b\" : \"bed and breakfast\",\n",
    "                        \"b.c\" : \"before christ\",\n",
    "                        \"b2b\" : \"business to business\",\n",
    "                        \"b2c\" : \"business to customer\",\n",
    "                        \"b4\" : \"before\",\n",
    "                        \"b4n\" : \"bye for now\",\n",
    "                        \"b@u\" : \"back at you\",\n",
    "                        \"bae\" : \"before anyone else\",\n",
    "                        \"bak\" : \"back at keyboard\",\n",
    "                        \"bbbg\" : \"bye bye be good\",\n",
    "                        \"bbc\" : \"british broadcasting corporation\",\n",
    "                        \"bbias\" : \"be back in a second\",\n",
    "                        \"bbl\" : \"be back later\",\n",
    "                        \"bbs\" : \"be back soon\",\n",
    "                        \"be4\" : \"before\",\n",
    "                        \"bfn\" : \"bye for now\",\n",
    "                        \"blvd\" : \"boulevard\",\n",
    "                        \"bout\" : \"about\",\n",
    "                        \"brb\" : \"be right back\",\n",
    "                        \"bros\" : \"brothers\",\n",
    "                        \"brt\" : \"be right there\",\n",
    "                        \"bsaaw\" : \"big smile and a wink\",\n",
    "                        \"btw\" : \"by the way\",\n",
    "                        \"bwl\" : \"bursting with laughter\",\n",
    "                        \"c/o\" : \"care of\",\n",
    "                        \"cet\" : \"central european time\",\n",
    "                        \"cf\" : \"compare\",\n",
    "                        \"cia\" : \"central intelligence agency\",\n",
    "                        \"csl\" : \"can not stop laughing\",\n",
    "                        \"cu\" : \"see you\",\n",
    "                        \"cul8r\" : \"see you later\",\n",
    "                        \"cv\" : \"curriculum vitae\",\n",
    "                        \"cwot\" : \"complete waste of time\",\n",
    "                        \"cya\" : \"see you\",\n",
    "                        \"cyt\" : \"see you tomorrow\",\n",
    "                        \"dae\" : \"does anyone else\",\n",
    "                        \"dbmib\" : \"do not bother me i am busy\",\n",
    "                        \"diy\" : \"do it yourself\",\n",
    "                        \"dm\" : \"direct message\",\n",
    "                        \"dwh\" : \"during work hours\",\n",
    "                        \"e123\" : \"easy as one two three\",\n",
    "                        \"eet\" : \"eastern european time\",\n",
    "                        \"eg\" : \"example\",\n",
    "                        \"embm\" : \"early morning business meeting\",\n",
    "                        \"encl\" : \"enclosed\",\n",
    "                        \"encl.\" : \"enclosed\",\n",
    "                        \"etc\" : \"and so on\",\n",
    "                        \"faq\" : \"frequently asked questions\",\n",
    "                        \"fawc\" : \"for anyone who cares\",\n",
    "                        \"fb\" : \"facebook\",\n",
    "                        \"fc\" : \"fingers crossed\",\n",
    "                        \"fig\" : \"figure\",\n",
    "                        \"fimh\" : \"forever in my heart\", \n",
    "                        \"ft.\" : \"feet\",\n",
    "                        \"ft\" : \"featuring\",\n",
    "                        \"ftl\" : \"for the loss\",\n",
    "                        \"ftw\" : \"for the win\",\n",
    "                        \"fwiw\" : \"for what it is worth\",\n",
    "                        \"fyi\" : \"for your information\",\n",
    "                        \"g9\" : \"genius\",\n",
    "                        \"gahoy\" : \"get a hold of yourself\",\n",
    "                        \"gal\" : \"get a life\",\n",
    "                        \"gcse\" : \"general certificate of secondary education\",\n",
    "                        \"gfn\" : \"gone for now\",\n",
    "                        \"gg\" : \"good game\",\n",
    "                        \"gl\" : \"good luck\",\n",
    "                        \"glhf\" : \"good luck have fun\",\n",
    "                        \"gmt\" : \"greenwich mean time\",\n",
    "                        \"gmta\" : \"great minds think alike\",\n",
    "                        \"gn\" : \"good night\",\n",
    "                        \"g.o.a.t\" : \"greatest of all time\",\n",
    "                        \"goat\" : \"greatest of all time\",\n",
    "                        \"goi\" : \"get over it\",\n",
    "                        \"gps\" : \"global positioning system\",\n",
    "                        \"gr8\" : \"great\",\n",
    "                        \"gratz\" : \"congratulations\",\n",
    "                        \"gyal\" : \"girl\",\n",
    "                        \"h&c\" : \"hot and cold\",\n",
    "                        \"hp\" : \"horsepower\",\n",
    "                        \"hr\" : \"hour\",\n",
    "                        \"hrh\" : \"his royal highness\",\n",
    "                        \"ht\" : \"height\",\n",
    "                        \"ibrb\" : \"i will be right back\",\n",
    "                        \"ic\" : \"i see\",\n",
    "                        \"icq\" : \"i seek you\",\n",
    "                        \"icymi\" : \"in case you missed it\",\n",
    "                        \"idc\" : \"i do not care\",\n",
    "                        \"idgadf\" : \"i do not give a damn fuck\",\n",
    "                        \"idgaf\" : \"i do not give a fuck\",\n",
    "                        \"idk\" : \"i do not know\",\n",
    "                        \"ie\" : \"that is\",\n",
    "                        \"i.e\" : \"that is\",\n",
    "                        \"ifyp\" : \"i feel your pain\",\n",
    "                        \"IG\" : \"instagram\",\n",
    "                        \"iirc\" : \"if i remember correctly\",\n",
    "                        \"ilu\" : \"i love you\",\n",
    "                        \"ily\" : \"i love you\",\n",
    "                        \"imho\" : \"in my humble opinion\",\n",
    "                        \"imo\" : \"in my opinion\",\n",
    "                        \"imu\" : \"i miss you\",\n",
    "                        \"iow\" : \"in other words\",\n",
    "                        \"irl\" : \"in real life\",\n",
    "                        \"j4f\" : \"just for fun\",\n",
    "                        \"jic\" : \"just in case\",\n",
    "                        \"jk\" : \"just kidding\",\n",
    "                        \"jsyk\" : \"just so you know\",\n",
    "                        \"l8r\" : \"later\",\n",
    "                        \"lb\" : \"pound\",\n",
    "                        \"lbs\" : \"pounds\",\n",
    "                        \"ldr\" : \"long distance relationship\",\n",
    "                        \"lmao\" : \"laugh my ass off\",\n",
    "                        \"lmfao\" : \"laugh my fucking ass off\",\n",
    "                        \"lol\" : \"laughing out loud\",\n",
    "                        \"ltd\" : \"limited\",\n",
    "                        \"ltns\" : \"long time no see\",\n",
    "                        \"m8\" : \"mate\",\n",
    "                        \"mf\" : \"motherfucker\",\n",
    "                        \"mfs\" : \"motherfuckers\",\n",
    "                        \"mfw\" : \"my face when\",\n",
    "                        \"mofo\" : \"motherfucker\",\n",
    "                        \"mph\" : \"miles per hour\",\n",
    "                        \"mr\" : \"mister\",\n",
    "                        \"mrw\" : \"my reaction when\",\n",
    "                        \"ms\" : \"miss\",\n",
    "                        \"mte\" : \"my thoughts exactly\",\n",
    "                        \"nagi\" : \"not a good idea\",\n",
    "                        \"nbc\" : \"national broadcasting company\",\n",
    "                        \"nbd\" : \"not big deal\",\n",
    "                        \"nfs\" : \"not for sale\",\n",
    "                        \"ngl\" : \"not going to lie\",\n",
    "                        \"nhs\" : \"national health service\",\n",
    "                        \"nrn\" : \"no reply necessary\",\n",
    "                        \"nsfl\" : \"not safe for life\",\n",
    "                        \"nsfw\" : \"not safe for work\",\n",
    "                        \"nth\" : \"nice to have\",\n",
    "                        \"nvr\" : \"never\",\n",
    "                        \"nyc\" : \"new york city\",\n",
    "                        \"oc\" : \"original content\",\n",
    "                        \"og\" : \"original\",\n",
    "                        \"ohp\" : \"overhead projector\",\n",
    "                        \"oic\" : \"oh i see\",\n",
    "                        \"omdb\" : \"over my dead body\",\n",
    "                        \"omg\" : \"oh my god\",\n",
    "                        \"omw\" : \"on my way\",\n",
    "                        \"p.a\" : \"per annum\",\n",
    "                        \"p.m\" : \"after midday\",\n",
    "                        \"pm\" : \"prime minister\",\n",
    "                        \"poc\" : \"people of color\",\n",
    "                        \"pov\" : \"point of view\",\n",
    "                        \"pp\" : \"pages\",\n",
    "                        \"ppl\" : \"people\",\n",
    "                        \"prw\" : \"parents are watching\",\n",
    "                        \"ps\" : \"postscript\",\n",
    "                        \"pt\" : \"point\",\n",
    "                        \"ptb\" : \"please text back\",\n",
    "                        \"pto\" : \"please turn over\",\n",
    "                        \"qpsa\" : \"what happens\", #\"que pasa\",\n",
    "                        \"ratchet\" : \"rude\",\n",
    "                        \"rbtl\" : \"read between the lines\",\n",
    "                        \"rlrt\" : \"real life retweet\", \n",
    "                        \"rofl\" : \"rolling on the floor laughing\",\n",
    "                        \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "                        \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "                        \"rt\" : \"retweet\",\n",
    "                        \"ruok\" : \"are you ok\",\n",
    "                        \"sfw\" : \"safe for work\",\n",
    "                        \"sk8\" : \"skate\",\n",
    "                        \"smh\" : \"shake my head\",\n",
    "                        \"sq\" : \"square\",\n",
    "                        \"srsly\" : \"seriously\", \n",
    "                        \"ssdd\" : \"same stuff different day\",\n",
    "                        \"tbh\" : \"to be honest\",\n",
    "                        \"tbs\" : \"tablespooful\",\n",
    "                        \"tbsp\" : \"tablespooful\",\n",
    "                        \"tfw\" : \"that feeling when\",\n",
    "                        \"thks\" : \"thank you\",\n",
    "                        \"tho\" : \"though\",\n",
    "                        \"thx\" : \"thank you\",\n",
    "                        \"tia\" : \"thanks in advance\",\n",
    "                        \"til\" : \"today i learned\",\n",
    "                        \"tl;dr\" : \"too long i did not read\",\n",
    "                        \"tldr\" : \"too long i did not read\",\n",
    "                        \"tmb\" : \"tweet me back\",\n",
    "                        \"tntl\" : \"trying not to laugh\",\n",
    "                        \"ttyl\" : \"talk to you later\",\n",
    "                        \"u\" : \"you\",\n",
    "                        \"u2\" : \"you too\",\n",
    "                        \"u4e\" : \"yours for ever\",\n",
    "                        \"utc\" : \"coordinated universal time\",\n",
    "                        \"w/\" : \"with\",\n",
    "                        \"w/o\" : \"without\",\n",
    "                        \"w8\" : \"wait\",\n",
    "                        \"wassup\" : \"what is up\",\n",
    "                        \"wb\" : \"welcome back\",\n",
    "                        \"wtf\" : \"what the fuck\",\n",
    "                        \"wtg\" : \"way to go\",\n",
    "                        \"wtpa\" : \"where the party at\",\n",
    "                        \"wuf\" : \"where are you from\",\n",
    "                        \"wuzup\" : \"what is up\",\n",
    "                        \"wywh\" : \"wish you were here\",\n",
    "                        \"yd\" : \"yard\",\n",
    "                        \"ygtr\" : \"you got that right\",\n",
    "                        \"ynk\" : \"you never know\",\n",
    "                        \"zzz\" : \"sleeping bored and tired\"\n",
    "                        }\n",
    "            \n",
    "        sample_typos_slang_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_typos_slang.keys()) + r')(?!\\w)')\n",
    "        sample_acronyms_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_acronyms.keys()) + r')(?!\\w)')\n",
    "        sample_abbr_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_abbr.keys()) + r')(?!\\w)')\n",
    "        \n",
    "        text = sample_typos_slang_pattern.sub(lambda x: sample_typos_slang[x.group()], text)\n",
    "        text = sample_acronyms_pattern.sub(lambda x: sample_acronyms[x.group()], text)\n",
    "        text = sample_abbr_pattern.sub(lambda x: sample_abbr[x.group()], text)\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "XinQSdMDz4eD"
   },
   "outputs": [],
   "source": [
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: informalAbbreviations_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "8I5fJ73U5e3_",
    "outputId": "5a7600c9-dca8-4572-d6d8-639825d6fcdc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>count_word</th>\n",
       "      <th>count_unique_word</th>\n",
       "      <th>count_letters</th>\n",
       "      <th>count_punctuations</th>\n",
       "      <th>count_stopwords</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>word_unique_percent</th>\n",
       "      <th>punct_percent</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>you_count</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Get fucking real dude.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.020690</td>\n",
       "      <td>0.012666</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>get fucking real dude .</td>\n",
       "      <td>[Get, fucking, real, dude, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>She is as dirty as they come  and that crook ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.158621</td>\n",
       "      <td>0.074266</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>4.080000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>she is as dirty as they come and that crook re...</td>\n",
       "      <td>[She, is, as, dirty, as, they, come, and, that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>why did you fuck it up. I could do it all day...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084795</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.074266</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>0.088542</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>86.666667</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.06250</td>\n",
       "      <td>why did you fuck it up . i i could do it all d...</td>\n",
       "      <td>[why, did, you, fuck, it, up, ., I, could, do,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dude they dont finish enclosing the fucking s...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058480</td>\n",
       "      <td>0.131034</td>\n",
       "      <td>0.071963</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>95.238095</td>\n",
       "      <td>19.047619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>dude they do not finish enclosing the fucking ...</td>\n",
       "      <td>[Dude, they, do, not, finish, enclosing, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>WTF are you talking about Men? No men thats n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040936</td>\n",
       "      <td>0.096552</td>\n",
       "      <td>0.042602</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.036458</td>\n",
       "      <td>3.933333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>what the fuck what the fuck are you talking ab...</td>\n",
       "      <td>[WTF, are, you, talking, about, Men, ?, No, me...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  label                                              tweet  \\\n",
       "0           0      1                             Get fucking real dude.   \n",
       "1           1      1   She is as dirty as they come  and that crook ...   \n",
       "2           2      1   why did you fuck it up. I could do it all day...   \n",
       "3           3      1   Dude they dont finish enclosing the fucking s...   \n",
       "4           4      1   WTF are you talking about Men? No men thats n...   \n",
       "\n",
       "   count_sent  count_word  count_unique_word  count_letters  \\\n",
       "0         0.0    0.008772           0.020690       0.012666   \n",
       "1         0.0    0.070175           0.158621       0.074266   \n",
       "2         0.0    0.084795           0.172414       0.074266   \n",
       "3         0.0    0.058480           0.131034       0.071963   \n",
       "4         0.0    0.040936           0.096552       0.042602   \n",
       "\n",
       "   count_punctuations  count_stopwords  mean_word_len  word_unique_percent  \\\n",
       "0            0.005208         0.000000       4.750000           100.000000   \n",
       "1            0.026042         0.062500       4.080000            96.000000   \n",
       "2            0.026042         0.088542       3.333333            86.666667   \n",
       "3            0.020833         0.026042       5.000000            95.238095   \n",
       "4            0.015625         0.036458       3.933333           100.000000   \n",
       "\n",
       "   punct_percent  num_exclamation_marks  num_question_marks  you_count  \\\n",
       "0      25.000000                    0.0            0.000000    0.00000   \n",
       "1      20.000000                    0.0            0.000000    0.00000   \n",
       "2      16.666667                    0.0            0.000000    0.06250   \n",
       "3      19.047619                    0.0            0.014706    0.00000   \n",
       "4      20.000000                    0.0            0.014706    0.03125   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0                            get fucking real dude .   \n",
       "1  she is as dirty as they come and that crook re...   \n",
       "2  why did you fuck it up . i i could do it all d...   \n",
       "3  dude they do not finish enclosing the fucking ...   \n",
       "4  what the fuck what the fuck are you talking ab...   \n",
       "\n",
       "                                           tokenized  \n",
       "0                      [Get, fucking, real, dude, .]  \n",
       "1  [She, is, as, dirty, as, they, come, and, that...  \n",
       "2  [why, did, you, fuck, it, up, ., I, could, do,...  \n",
       "3  [Dude, they, do, not, finish, enclosing, the, ...  \n",
       "4  [WTF, are, you, talking, about, Men, ?, No, me...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "jVlEb_VUbxNG",
    "outputId": "9276c520-aaf0-467b-c64d-e046eea237f1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>count_word</th>\n",
       "      <th>count_unique_word</th>\n",
       "      <th>count_letters</th>\n",
       "      <th>count_punctuations</th>\n",
       "      <th>count_stopwords</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>word_unique_percent</th>\n",
       "      <th>punct_percent</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>you_count</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Get fucking real dude.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.020690</td>\n",
       "      <td>0.012666</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>get fucking real dude .</td>\n",
       "      <td>[Get, fucking, real, dude, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>She is as dirty as they come  and that crook ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.158621</td>\n",
       "      <td>0.074266</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>4.080000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>she is as dirty as they come and that crook re...</td>\n",
       "      <td>[She, is, as, dirty, as, they, come, and, that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>why did you fuck it up. I could do it all day...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084795</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.074266</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>0.088542</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>86.666667</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.06250</td>\n",
       "      <td>why did you fuck it up . i i could do it all d...</td>\n",
       "      <td>[why, did, you, fuck, it, up, ., I, could, do,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dude they dont finish enclosing the fucking s...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058480</td>\n",
       "      <td>0.131034</td>\n",
       "      <td>0.071963</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>95.238095</td>\n",
       "      <td>19.047619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>dude they do not finish enclosing the fucking ...</td>\n",
       "      <td>[Dude, they, do, not, finish, enclosing, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>WTF are you talking about Men? No men thats n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040936</td>\n",
       "      <td>0.096552</td>\n",
       "      <td>0.042602</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.036458</td>\n",
       "      <td>3.933333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>what the fuck what the fuck are you talking ab...</td>\n",
       "      <td>[WTF, are, you, talking, about, Men, ?, No, me...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  label                                              tweet  \\\n",
       "0           0      1                             Get fucking real dude.   \n",
       "1           1      1   She is as dirty as they come  and that crook ...   \n",
       "2           2      1   why did you fuck it up. I could do it all day...   \n",
       "3           3      1   Dude they dont finish enclosing the fucking s...   \n",
       "4           4      1   WTF are you talking about Men? No men thats n...   \n",
       "\n",
       "   count_sent  count_word  count_unique_word  count_letters  \\\n",
       "0         0.0    0.008772           0.020690       0.012666   \n",
       "1         0.0    0.070175           0.158621       0.074266   \n",
       "2         0.0    0.084795           0.172414       0.074266   \n",
       "3         0.0    0.058480           0.131034       0.071963   \n",
       "4         0.0    0.040936           0.096552       0.042602   \n",
       "\n",
       "   count_punctuations  count_stopwords  mean_word_len  word_unique_percent  \\\n",
       "0            0.005208         0.000000       4.750000           100.000000   \n",
       "1            0.026042         0.062500       4.080000            96.000000   \n",
       "2            0.026042         0.088542       3.333333            86.666667   \n",
       "3            0.020833         0.026042       5.000000            95.238095   \n",
       "4            0.015625         0.036458       3.933333           100.000000   \n",
       "\n",
       "   punct_percent  num_exclamation_marks  num_question_marks  you_count  \\\n",
       "0      25.000000                    0.0            0.000000    0.00000   \n",
       "1      20.000000                    0.0            0.000000    0.00000   \n",
       "2      16.666667                    0.0            0.000000    0.06250   \n",
       "3      19.047619                    0.0            0.014706    0.00000   \n",
       "4      20.000000                    0.0            0.014706    0.03125   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0                            get fucking real dude .   \n",
       "1  she is as dirty as they come and that crook re...   \n",
       "2  why did you fuck it up . i i could do it all d...   \n",
       "3  dude they do not finish enclosing the fucking ...   \n",
       "4  what the fuck what the fuck are you talking ab...   \n",
       "\n",
       "                                           tokenized  \n",
       "0                      [Get, fucking, real, dude, .]  \n",
       "1  [She, is, as, dirty, as, they, come, and, that...  \n",
       "2  [why, did, you, fuck, it, up, ., I, could, do,...  \n",
       "3  [Dude, they, do, not, finish, enclosing, the, ...  \n",
       "4  [WTF, are, you, talking, about, Men, ?, No, me...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQptCVO3sHbH"
   },
   "source": [
    "### Remove non ascii charcters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "hWVtKJQOr9gX"
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "\n",
    "    \"\"\"\n",
    "        Remove non-ASCII characters \n",
    "    \"\"\"\n",
    "\n",
    "    return re.sub(r'[^\\x00-\\x7f]',r'', text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "12PeMtcer9qS"
   },
   "outputs": [],
   "source": [
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_non_ascii(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDA5QDoKsN9c"
   },
   "source": [
    "### Remove any other special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "aK4HiF1ctyaM"
   },
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    \"\"\"\n",
    "        Remove special special characters, including symbols, emojis, and other graphic characters\n",
    "    \"\"\"\n",
    "    emoji_pattern = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "jJ-nIi9lt4q9"
   },
   "outputs": [],
   "source": [
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_special_characters(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdFa7rJ4scmX"
   },
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "Cd5lpBLMt7gF"
   },
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    \"\"\"\n",
    "        Remove the punctuation\n",
    "    \"\"\"\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "Q3ba7b_qvC_y"
   },
   "outputs": [],
   "source": [
    "train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkITUuQQCBGQ"
   },
   "source": [
    "### Remove empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "uueNAwOTCJ-X"
   },
   "outputs": [],
   "source": [
    "train_df[\"text_clean\"].replace('', np.nan, inplace=True)\n",
    "train_df.dropna(subset=[\"text_clean\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfOjDB_lhZRL"
   },
   "source": [
    "\n",
    "# Text Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bAT3-7Xtw7_"
   },
   "source": [
    "### **Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "NSlKvw5IhYAp",
    "outputId": "6c21968f-c814-4747-8075-a3f56a71c0fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\.conda\\envs\\gpu\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: regex in c:\\users\\lenovo\\.conda\\envs\\gpu\\lib\\site-packages (from nltk) (2021.3.17)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\.conda\\envs\\gpu\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\.conda\\envs\\gpu\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\.conda\\envs\\gpu\\lib\\site-packages (from nltk) (4.59.0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>count_word</th>\n",
       "      <th>count_unique_word</th>\n",
       "      <th>count_letters</th>\n",
       "      <th>count_punctuations</th>\n",
       "      <th>count_stopwords</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>word_unique_percent</th>\n",
       "      <th>punct_percent</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>you_count</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Get fucking real dude.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.020690</td>\n",
       "      <td>0.012666</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>get fucking real dude</td>\n",
       "      <td>[get, fucking, real, dude]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>She is as dirty as they come  and that crook ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.158621</td>\n",
       "      <td>0.074266</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>4.080000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>she is as dirty as they come and that crook re...</td>\n",
       "      <td>[she, is, as, dirty, as, they, come, and, that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>why did you fuck it up. I could do it all day...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084795</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.074266</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>0.088542</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>86.666667</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.06250</td>\n",
       "      <td>why did you fuck it up  i i could do it all da...</td>\n",
       "      <td>[why, did, you, fuck, it, up, i, i, could, do,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dude they dont finish enclosing the fucking s...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058480</td>\n",
       "      <td>0.131034</td>\n",
       "      <td>0.071963</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>95.238095</td>\n",
       "      <td>19.047619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>dude they do not finish enclosing the fucking ...</td>\n",
       "      <td>[dude, they, do, not, finish, enclosing, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>WTF are you talking about Men? No men thats n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040936</td>\n",
       "      <td>0.096552</td>\n",
       "      <td>0.042602</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.036458</td>\n",
       "      <td>3.933333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>what the fuck what the fuck are you talking ab...</td>\n",
       "      <td>[what, the, fuck, what, the, fuck, are, you, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  label                                              tweet  \\\n",
       "0           0      1                             Get fucking real dude.   \n",
       "1           1      1   She is as dirty as they come  and that crook ...   \n",
       "2           2      1   why did you fuck it up. I could do it all day...   \n",
       "3           3      1   Dude they dont finish enclosing the fucking s...   \n",
       "4           4      1   WTF are you talking about Men? No men thats n...   \n",
       "\n",
       "   count_sent  count_word  count_unique_word  count_letters  \\\n",
       "0         0.0    0.008772           0.020690       0.012666   \n",
       "1         0.0    0.070175           0.158621       0.074266   \n",
       "2         0.0    0.084795           0.172414       0.074266   \n",
       "3         0.0    0.058480           0.131034       0.071963   \n",
       "4         0.0    0.040936           0.096552       0.042602   \n",
       "\n",
       "   count_punctuations  count_stopwords  mean_word_len  word_unique_percent  \\\n",
       "0            0.005208         0.000000       4.750000           100.000000   \n",
       "1            0.026042         0.062500       4.080000            96.000000   \n",
       "2            0.026042         0.088542       3.333333            86.666667   \n",
       "3            0.020833         0.026042       5.000000            95.238095   \n",
       "4            0.015625         0.036458       3.933333           100.000000   \n",
       "\n",
       "   punct_percent  num_exclamation_marks  num_question_marks  you_count  \\\n",
       "0      25.000000                    0.0            0.000000    0.00000   \n",
       "1      20.000000                    0.0            0.000000    0.00000   \n",
       "2      16.666667                    0.0            0.000000    0.06250   \n",
       "3      19.047619                    0.0            0.014706    0.00000   \n",
       "4      20.000000                    0.0            0.014706    0.03125   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0                             get fucking real dude    \n",
       "1  she is as dirty as they come and that crook re...   \n",
       "2  why did you fuck it up  i i could do it all da...   \n",
       "3  dude they do not finish enclosing the fucking ...   \n",
       "4  what the fuck what the fuck are you talking ab...   \n",
       "\n",
       "                                           tokenized  \n",
       "0                         [get, fucking, real, dude]  \n",
       "1  [she, is, as, dirty, as, they, come, and, that...  \n",
       "2  [why, did, you, fuck, it, up, i, i, could, do,...  \n",
       "3  [dude, they, do, not, finish, enclosing, the, ...  \n",
       "4  [what, the, fuck, what, the, fuck, are, you, t...  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "train_df[\"tokenized\"] = train_df[\"text_clean\"].apply(word_tokenize)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHM4_fkLhOE9"
   },
   "source": [
    "\n",
    "### **Remove stop word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "My_stopWordList=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S57840OahNVx",
    "outputId": "81020f56-f84e-44a0-be7b-a86a44bfe593"
   },
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#stop = set(stopwords.words('english'))\n",
    "train_df['stopwords_removed'] = train_df[\"tokenized\"].apply(lambda x: [word for word in x if word not in My_stopWordList])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEAsIAOmjplO"
   },
   "source": [
    "### **Part of Speech Tagging (POS Tagging):**\n",
    "\n",
    "Part of speech tagging (POS tagging) distinguishes the part of speech (noun, verb, adjective, and etc.) of each word in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X93d9jNBjpKw",
    "outputId": "a5946792-8431-4ccb-9711-01016e8231f9"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import brown\n",
    "\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \n",
    "               \"V\":wordnet.VERB, \n",
    "               \"J\":wordnet.ADJ, \n",
    "               \"R\":wordnet.ADV\n",
    "              }\n",
    "    \n",
    "train_sents = brown.tagged_sents(categories='news')\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "\n",
    "def pos_tag_wordnet(text, pos_tag_type=\"pos_tag\"):\n",
    "    \"\"\"\n",
    "        Create pos_tag with wordnet format\n",
    "    \"\"\"\n",
    "    pos_tagged_text = t2.tag(text)\n",
    "    \n",
    "    # map the pos tagging output with wordnet output \n",
    "    pos_tagged_text = [(word, wordnet_map.get(pos_tag[0])) if pos_tag[0] in wordnet_map.keys() else (word, wordnet.NOUN) for (word, pos_tag) in pos_tagged_text ]\n",
    "    return pos_tagged_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "PsxJo-ZMjzed"
   },
   "outputs": [],
   "source": [
    "train_df['pos_tag'] = train_df['stopwords_removed'].apply(lambda x: pos_tag_wordnet(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovWC2bDEj8BS"
   },
   "source": [
    "### **Lemmatization:**\n",
    "\n",
    "Lemmatization is the task of determining that two words have the same root, despite their surface differences. The words am, are, and is have the shared lemma be; the words dinner and dinners both have the lemma dinner. \n",
    "Lemmatizing each of these forms to the same lemma will let us ï¬nd all mentions of words in Russian like Moscow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "21FBhMGsS0t1"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    \"\"\"\n",
    "        Lemmatize the tokenized words\n",
    "    \"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma = [lemmatizer.lemmatize(word, tag) for word, tag in text]\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import WordNetLemmatizer as form_replacer\n",
    "# lemmatized_irr2 = list(set([form_replacer().lemmatize(x) for x in ['isis','one']]))\n",
    "# print(lemmatized_irr2 )\n",
    "# #apply(lambda x: [lemmatizer.lemmatize(word) for word in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "Y-AQijDVkNRs"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "train_df['lemmatize_word_w_pos'] = train_df['pos_tag'].apply(lambda x: lemmatize_word(x))\n",
    "train_df['lemmatize_text'] = [' '.join(map(str, l)) for l in train_df['lemmatize_word_w_pos']] # join back to text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UtI18Fft6pA"
   },
   "source": [
    "### **Saving Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "oPgGeR4j9NKz"
   },
   "outputs": [],
   "source": [
    "train_df.to_csv('Clean30k_features.csv')# choose the name to be saved with "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GP Model 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
