{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import seaborn as sns\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from keras import backend as K\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.svm import SVC\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.metrics import roc_auc_score, r2_score\r\n",
    "from keras.optimizers import Adam\r\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\r\n",
    "from sklearn.model_selection import KFold \r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, GRU, Embedding, Dropout, Activation,Bidirectional, GlobalMaxPool1D, Flatten\r\n",
    "from tensorflow.keras.models import Model\r\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\r\n",
    "import os, string\r\n",
    "from tensorflow.keras import callbacks\r\n",
    "from keras.callbacks import Callback\r\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from nltk.corpus import stopwords\r\n",
    "stopwords = set(stopwords.words(\"english\"))\r\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "EMBEDDING_FILE_FASTTEXT=\"D:/FastText/crawl-300d-2M.vec/crawl-300d-2M.vec\"\r\n",
    "EMBEDDING_FILE_TWITTER=\"D:/glove.twitter.27B/glove.twitter.27B.200d.txt\"\r\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\r\n",
    "embeddings_index_ft = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE_FASTTEXT,encoding='utf-8'))\r\n",
    "embeddings_index_tw = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE_TWITTER,encoding='utf-8'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "cols_features = ['count_word','capitals', 'count_unique_word',  'count_punctuations','word_unique_percent' ,'punct_percent'\r\n",
    "                 , 'num_exclamation_marks', 'num_question_marks', 'you_count','mentions','smilies' ,'symbols']\r\n",
    "\r\n",
    "\r\n",
    "def get_features(df):\r\n",
    "    \r\n",
    "    df['count_word']=df[\"tweet\"].apply(lambda comment: len(str(comment).split()))\r\n",
    "    df['capitals'] = df[\"tweet\"].apply(lambda comment: sum(1 for c in str(comment) if c.isupper()))\r\n",
    "    df['count_unique_word']=df[\"tweet\"].apply(lambda comment: len(set(str(comment).split())))\r\n",
    "    df[\"count_punctuations\"] =df[\"tweet\"].apply(lambda comment: len([c for c in str(comment) if c in  string.punctuation]))#try remove\r\n",
    "    df['word_unique_percent']=df['count_unique_word']*100/df['count_word']\r\n",
    "    df['punct_percent']=df['count_punctuations']*100/df['count_word']\r\n",
    "    df['num_exclamation_marks'] = df['tweet'].apply(lambda comment: str(comment).count('!'))\r\n",
    "    df['num_question_marks'] = df['tweet'].apply(lambda comment:  str(comment).count('?'))\r\n",
    "    df['you_count'] = df['tweet'].apply(lambda comment: sum( str(comment).count(w) for w in ('you', 'You', 'YOU')))\r\n",
    "    df['mentions'] = df['tweet'].apply(lambda comment: str(comment).count(\"@\"))\r\n",
    "    df['smilies'] = df['tweet'].apply(lambda comment: sum(str(comment).count(w) for w in (':-)', ':)', ';-)', ';)')))#try remove\r\n",
    "    df['symbols'] = df['tweet'].apply(lambda comment: sum(str(comment).count(w) for w in '*&#$%“”¨«»®´·º½¾¿¡§£₤‘’'))\r\n",
    "    \r\n",
    "    scaler = MinMaxScaler().fit(df[cols_features])\r\n",
    "    df[cols_features] = scaler.transform(df[cols_features])\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df =  pd.read_csv('C:/Users/Lenovo/Desktop/GP Code/CleaneddroppedCivil60K2/CleaneddroppedCivil60K2.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df = get_features(train_df)\r\n",
    "train_df[\"toxicity\"] = train_df[\"toxicity\"].apply(lambda x: 1 if x>=0.5 else 0)\r\n",
    "train_df.head(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cols = ['lemmatize_text','count_word','capitals', 'count_unique_word',  'count_punctuations','word_unique_percent' ,'punct_percent'\r\n",
    "                 , 'num_exclamation_marks', 'num_question_marks', 'you_count','mentions','smilies' ,'symbols']\r\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_df[cols], train_df['toxicity'].values, test_size=0.2, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_test = x_test['lemmatize_text'].astype(str)\r\n",
    "X = x_train['lemmatize_text'].astype(str)\r\n",
    "X_feat = x_train[cols_features]\r\n",
    "Y = y_train\r\n",
    "Test_feat = x_test[cols_features]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "tokenizer = Tokenizer( oov_token = True)\r\n",
    "tokenizer.fit_on_texts(list(X))\r\n",
    "X_train_sequence = tokenizer.texts_to_sequences(X)\r\n",
    "X_test_sequence = tokenizer.texts_to_sequences(X_test)\r\n",
    "max_features = len(tokenizer.word_index) + 1\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def FindMaxLength(lst):\r\n",
    "    maxList = max((x) for x in lst)\r\n",
    "    maxLength = max(len(x) for x in lst )\r\n",
    "  \r\n",
    "    return maxList, maxLength"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# find the longest sequence\r\n",
    "_, maxlen1 = FindMaxLength(X_train_sequence)\r\n",
    "_, maxlen2 = FindMaxLength(X_test_sequence )\r\n",
    "maxlen = max(maxlen2 ,maxlen1)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from itertools import cycle\r\n",
    "\r\n",
    "def list_padding(lst , maxlen):\r\n",
    "    lstNew = lst.copy()\r\n",
    "    myiter = cycle(lstNew)\r\n",
    "    for _ in range(maxlen):\r\n",
    "        lstNew.append(next(myiter))\r\n",
    "    return lstNew"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def custom_padding(lst, maxlength):\r\n",
    "    new =[]\r\n",
    "    for  i in  range(len(lst)):\r\n",
    "        length =  maxlength - len(lst[i])\r\n",
    "        my_padding = list_padding(lst[i] , length)\r\n",
    "        new.append( my_padding)\r\n",
    "    return new    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "padTrain =custom_padding(X_train_sequence, maxlen)\r\n",
    "padTest = custom_padding(X_test_sequence, maxlen)\r\n",
    "x_train_pad = pad_sequences(padTrain )\r\n",
    "x_test_pad  = pad_sequences(padTest )\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(max_features)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "word_index = tokenizer.word_index\r\n",
    "nb_words = max_features\r\n",
    "embedding_matrix = np.zeros((nb_words,500))\r\n",
    "dim =500\r\n",
    "\r\n",
    "something_tw = embeddings_index_tw.get(\"something\")\r\n",
    "something_ft = embeddings_index_ft.get(\"something\")\r\n",
    "\r\n",
    "something = np.zeros((500,))\r\n",
    "something[:300,] = something_ft\r\n",
    "something[300:500,] = something_tw\r\n",
    "# we define here embedding vector for word \"something\" whenever the model founds word doesnot have vector representing \r\n",
    "#in both fasttext and glove\r\n",
    "\r\n",
    "\r\n",
    "def embed_word(embedding_matrix,i,word):\r\n",
    "    embedding_vector_ft = embeddings_index_ft.get(word)\r\n",
    "    if embedding_vector_ft is not None: \r\n",
    "        embedding_matrix[i,:300] = embedding_vector_ft\r\n",
    "        embedding_vector_tw = embeddings_index_tw.get(word)\r\n",
    "        if embedding_vector_tw is not None:\r\n",
    "            embedding_matrix[i,300:500] = embedding_vector_tw\r\n",
    "#here we define to arrange fasttext first 300 and then glove the second 200\r\n",
    "            \r\n",
    "for word, i in word_index.items():            \r\n",
    "    if embeddings_index_ft.get(word) is not None:\r\n",
    "        embed_word(embedding_matrix,i,word)\r\n",
    "    else:\r\n",
    "        embedding_matrix[i] = something\r\n",
    "# we loop over all unique words in word-index_items  \r\n",
    "# if the there is unique word doenot have glove nor fasttext vector we put \"something\" vector representing for that word"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from keras import backend as K\r\n",
    "from keras.engine.topology import Layer\r\n",
    "from keras import initializers, regularizers, constraints\r\n",
    "\r\n",
    "\r\n",
    "class attention(Layer):\r\n",
    "    def __init__(self,**kwargs):\r\n",
    "        super(attention,self).__init__(**kwargs)\r\n",
    "\r\n",
    "    def build(self,input_shape):\r\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\r\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \r\n",
    "        super(attention, self).build(input_shape)\r\n",
    "\r\n",
    "    def call(self,x):\r\n",
    "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\r\n",
    "        at=K.softmax(et)\r\n",
    "        at=K.expand_dims(at,axis=-1)\r\n",
    "        output=x*at\r\n",
    "        return K.sum(output,axis=1)\r\n",
    "\r\n",
    "    def compute_output_shape(self,input_shape):\r\n",
    "        return (input_shape[0],input_shape[-1])\r\n",
    "\r\n",
    "    def get_config(self):\r\n",
    "        return super(attention,self).get_config()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "inp = Input(shape = (maxlen,))\r\n",
    "x = Embedding(max_features, dim , weights = [embedding_matrix], trainable = False)(inp)\r\n",
    "x = Bidirectional(GRU(20, return_sequences=True))(x)\r\n",
    "x = attention()(x)\r\n",
    "features = Input(shape=(len(cols_features),))\r\n",
    "conc = concatenate([x, features])\r\n",
    "x= Flatten(name='Flatten')(x)\r\n",
    "conc = Dropout(0.2)(x)\r\n",
    "preds = Dense(1, activation=\"sigmoid\")(conc)\r\n",
    "model = Model(inputs=[inp, features], outputs=preds)\r\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(clipvalue=1, clipnorm=1), metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "K.clear_session()\r\n",
    "k = 10\r\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=239)\r\n",
    "acc_score = []\r\n",
    "X_Flod = x_train_pad\r\n",
    "y_Flod = Y\r\n",
    "feat_Flod = X_feat.values\r\n",
    "\r\n",
    "for train_index , test_index in kf.split(X):\r\n",
    "\r\n",
    "    X_train , X_valid = X_Flod[train_index],  X_Flod[test_index]\r\n",
    "    y_train , y_valid =y_Flod[train_index] ,y_Flod[test_index]\r\n",
    "    feat_Train ,feat_test =  feat_Flod[train_index], feat_Flod[test_index]\r\n",
    "    K.clear_session()\r\n",
    "    hist = model.fit([X_train,feat_Train],y_train,epochs=1,batch_size = 32,verbose=1)\r\n",
    "    pred_values = model.predict([X_valid,feat_test],batch_size = 32,verbose=1)\r\n",
    "    pred_values*=100\r\n",
    "    pred_values=[1 if i >50  else 0 for i in pred_values ]\r\n",
    "    acc = accuracy_score(pred_values ,y_valid)\r\n",
    "    acc_score.append(acc)\r\n",
    "     \r\n",
    "avg_acc_score = sum(acc_score)/k\r\n",
    " \r\n",
    "print('accuracy of each fold - {}'.format(acc_score))\r\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prediction = model.evaluate( [x_test_pad,Test_feat],y_test,batch_size = 32,verbose=1)\r\n",
    "print(f'Test loss: {prediction[0]} / Test accuracy: {prediction[1]}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_df =  pd.read_csv('C:/Users/Lenovo/Desktop/GP Code/CleanTest.csv')\r\n",
    "test_df = get_features(test_df )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_df.head(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "testX = test_df['lemmatize_text'].astype(str)\r\n",
    "testY =test_df['label']\r\n",
    "testFeat = test_df[cols_features]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "testX_sequence = tokenizer.texts_to_sequences(testX)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "padTesting=custom_padding(testX_sequence, maxlen)\r\n",
    "testX_pad  = pad_sequences(padTesting)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pred_v = model.predict([testX_pad,testFeat])\r\n",
    "pred_v*=100"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pred_v=[1 if i >20  else 0 for i in pred_v ]\r\n",
    "print(\"predictied values\\n\")\r\n",
    "print(pred_v)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"expected values\\n\")\r\n",
    "print(\"[0, 1, 0, 0, 1, 1, 1 , 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0]\"   )          "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}