{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GP Model 2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOIr7IdTD6LXnncgIB1rNiE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"POQpUq40q8k5","executionInfo":{"status":"ok","timestamp":1619584151932,"user_tz":-120,"elapsed":7430,"user":{"displayName":"Nancy Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfzroUWqthIy7jUUOd9jfaIhO9mtFnKpf8FI7FpQ=s64","userId":"08005434482666482410"}},"outputId":"7be6b9f5-28f2-4456-93a9-7e4ccdb86e5a"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import keras\n","import tensorflow as tf\n","from keras.models import Model, Sequential\n","from keras.layers import Dense, Embedding, Input,  Activation\n","from keras.layers import  LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras import initializers, optimizers, layers\n","from sklearn.metrics import roc_auc_score\n","from keras.models import Sequential\n","from keras.initializers import Constant\n","from keras.callbacks import ModelCheckpoint\n","from keras.models import load_model\n","import pandas as pd\n","import io \n","import re                                  # library for regular expression operations\n","import string \n","!pip install emoji                             # for string operations\n","import emoji\n","from nltk.corpus import stopwords          # module for stop words that come with NLTK\n","from nltk.stem import PorterStemmer        # module for stemming\n","from nltk.tokenize import regexp_tokenize   # module for tokenizing strings\n","from nltk.tokenize import TreebankWordTokenizer\n","import nltk \n","import numpy as np\n","!pip install WordCloud\n","from wordcloud import WordCloud, STOPWORDS\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from tensorflow.python.keras.utils import layer_utils\n","from keras import backend as K\n","from keras.engine.topology import Layer\n","from keras import initializers, regularizers, constraints\n","from keras.models import Sequential"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.2.0)\n","Requirement already satisfied: WordCloud in /usr/local/lib/python3.7/dist-packages (1.5.0)\n","Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from WordCloud) (1.19.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from WordCloud) (7.1.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qi1Q54KvtV9h","executionInfo":{"status":"ok","timestamp":1619584164418,"user_tz":-120,"elapsed":5961,"user":{"displayName":"Nancy Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfzroUWqthIy7jUUOd9jfaIhO9mtFnKpf8FI7FpQ=s64","userId":"08005434482666482410"}},"outputId":"fe6451a3-cad9-41e4-b843-380318f97c60"},"source":["import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"id":"zKq9SfCjqAkT","executionInfo":{"status":"ok","timestamp":1619584164419,"user_tz":-120,"elapsed":4064,"user":{"displayName":"Nancy Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfzroUWqthIy7jUUOd9jfaIhO9mtFnKpf8FI7FpQ=s64","userId":"08005434482666482410"}},"outputId":"75d0d5bd-58e6-438c-80ee-99c9b7f53f23"},"source":["train_df = pd.read_csv(\"train.csv\")# twitter comments\n","train_df.head()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>@user when a father is dysfunctional and is s...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>@user @user thanks for #lyft credit i can't us...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>bihday your majesty</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>#model   i love u take with u all the time in ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>factsguide: society now    #motivation</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  label                                              tweet\n","0   1      0   @user when a father is dysfunctional and is s...\n","1   2      0  @user @user thanks for #lyft credit i can't us...\n","2   3      0                                bihday your majesty\n","3   4      0  #model   i love u take with u all the time in ...\n","4   5      0             factsguide: society now    #motivation"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"RgvB2kbWqWKh"},"source":["# Clean Data\n"]},{"cell_type":"markdown","metadata":{"id":"Z7kn7vX4ox-y"},"source":["###Remove @ username in Twitter"]},{"cell_type":"code","metadata":{"id":"6hOGd94tsL8n"},"source":["def remove_at_user(text):\n","\n","  \"\"\"\n","      Remove @username from tweets\n","  \"\"\"\n","  \n","  return re.sub(\"@[A-Za-z0-9]+\",\"\",text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hjhj4Vv5tSzj"},"source":["train_df[\"text_clean\"] = train_df[\"tweet\"].apply(lambda x: remove_at_user(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QHdZgf8Sgoi5"},"source":["### Repeat the captial words \n","Here we repeat the Captial words to confirm the meaning of this word as the user try to focus on it\n"]},{"cell_type":"code","metadata":{"id":"l8zIkM2igy5i"},"source":["!pip install nltk\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","train_df['tokenized'] = train_df[\"text_clean\"].apply(word_tokenize)\n","train_df.head()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7BqLRrIIaoa","executionInfo":{"status":"ok","timestamp":1619585294002,"user_tz":-120,"elapsed":3370,"user":{"displayName":"Nancy Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfzroUWqthIy7jUUOd9jfaIhO9mtFnKpf8FI7FpQ=s64","userId":"08005434482666482410"}}},"source":["train_df['text_clean']=train_df['tokenized'].apply(lambda x:  [word  if word.isupper() == False else word+\" \"+word  for word in x]) \n","train_df['text_clean'] = [' '.join(map(str, l)) for l in train_df['text_clean']] # join back to text"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"id":"XSid1k5Tl267","executionInfo":{"status":"ok","timestamp":1619585303274,"user_tz":-120,"elapsed":8347,"user":{"displayName":"Nancy Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfzroUWqthIy7jUUOd9jfaIhO9mtFnKpf8FI7FpQ=s64","userId":"08005434482666482410"}},"outputId":"81d6e09c-0035-4a4e-ad99-f6915bfda2e9"},"source":["\n","train_df.head()"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>tweet</th>\n","      <th>tokenized</th>\n","      <th>Capatial</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>@user when a father is dysfunctional and is s...</td>\n","      <td>[@, user, when, a, father, is, dysfunctional, ...</td>\n","      <td>@ user when a father is dysfunctional and is s...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>@user @user thanks for #lyft credit i can't us...</td>\n","      <td>[@, user, @, user, thanks, for, #, lyft, credi...</td>\n","      <td>@ user @ user thanks for # lyft credit i ca n'...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>@Nancy SHUT UP your mouth</td>\n","      <td>[@, Nancy, SHUT, UP, your, mouth]</td>\n","      <td>@ Nancy SHUT SHUT UP UP your mouth</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>#model   i love u take with u all the time in ...</td>\n","      <td>[#, model, i, love, u, take, with, u, all, the...</td>\n","      <td># model i love u take with u all the time in u...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>factsguide: society now    #motivation</td>\n","      <td>[factsguide, :, society, now, #, motivation]</td>\n","      <td>factsguide : society now # motivation</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  ...                                           Capatial\n","0   1  ...  @ user when a father is dysfunctional and is s...\n","1   2  ...  @ user @ user thanks for # lyft credit i ca n'...\n","2   3  ...                 @ Nancy SHUT SHUT UP UP your mouth\n","3   4  ...  # model i love u take with u all the time in u...\n","4   5  ...              factsguide : society now # motivation\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"jdh6w0c9gzki"},"source":["###  Covert to lower case"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":196},"id":"auI4cFbvqbCY","executionInfo":{"status":"ok","timestamp":1618442280356,"user_tz":-120,"elapsed":674,"user":{"displayName":"Nancy Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfzroUWqthIy7jUUOd9jfaIhO9mtFnKpf8FI7FpQ=s64","userId":"08005434482666482410"}},"outputId":"d4152a3b-917f-45c8-9bb7-6a320fa49773"},"source":["train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: x.lower())\n","display(train_df.head())"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>@user when a father is dysfunctional and is s...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>@user @user thanks for #lyft credit i can't us...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>bihday your majesty</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>#model   i love u take with u all the time in ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>factsguide: society now    #motivation</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  label                                              tweet\n","0   1      0   @user when a father is dysfunctional and is s...\n","1   2      0  @user @user thanks for #lyft credit i can't us...\n","2   3      0                                bihday your majesty\n","3   4      0  #model   i love u take with u all the time in ...\n","4   5      0             factsguide: society now    #motivation"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"9b1XThQboDLi"},"source":["###  Contractions \n","\n","We use the contractions package to expand the contraction in English such as we'll -> we will or we shouldn't've -> we should not have."]},{"cell_type":"code","metadata":{"id":"SdIiCz30q_c0"},"source":["!pip install contractions\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kx5ic9egrlK-"},"source":["import contractions\n","train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: contractions.fix(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yvy5NH5CojyJ"},"source":["### Remove any URLS"]},{"cell_type":"code","metadata":{"id":"uIHYHjOtrMIc"},"source":["def remove_URL(text):\n","    \"\"\"\n","        Remove URLs from a sample string\n","    \"\"\"\n","\n","    return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m3d4iMiNrPn0"},"source":["train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_URL(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DFejSgwJos7I"},"source":["### Remove HTML Tages"]},{"cell_type":"code","metadata":{"id":"w-DOPNF2r27r"},"source":["def remove_html(text):\n","    \"\"\"\n","        Remove the html in sample text\n","    \"\"\"\n","    html = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n","    return re.sub(html, \"\", text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sH3wQ3Emr5vP"},"source":["train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_html(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B1Weuyiwo9NE"},"source":["### Replace the Typos, slang, acronyms or informal abbreviations"]},{"cell_type":"code","metadata":{"id":"qQwTF4UWvFuq"},"source":["def informalAbbreviations_clean(text):\n","        \"\"\"\n","            Other manual text cleaning techniques\n","        \"\"\"\n","        # Typos, slang and other\n","        sample_typos_slang = {\n","                                \"w/e\": \"whatever\",\n","                                \"usagov\": \"usa government\",\n","                                \"recentlu\": \"recently\",\n","                                \"ph0tos\": \"photos\",\n","                                \"amirite\": \"am i right\",\n","                                \"exp0sed\": \"exposed\",\n","                                \"<3\": \"love\",\n","                                \"luv\": \"love\",\n","                                \"amageddon\": \"armageddon\",\n","                                \"trfc\": \"traffic\",\n","                                \"16yr\": \"16 year\"\n","                                }\n","\n","        # Acronyms\n","        sample_acronyms =  { \n","                            \"mh370\": \"malaysia airlines flight 370\",\n","                            \"okwx\": \"oklahoma city weather\",\n","                            \"arwx\": \"arkansas weather\",    \n","                            \"gawx\": \"georgia weather\",  \n","                            \"scwx\": \"south carolina weather\",  \n","                            \"cawx\": \"california weather\",\n","                            \"tnwx\": \"tennessee weather\",\n","                            \"azwx\": \"arizona weather\",  \n","                            \"alwx\": \"alabama weather\",\n","                            \"usnwsgov\": \"united states national weather service\",\n","                            \"2mw\": \"tomorrow\"\n","                            }\n","\n","        \n","        # Some common abbreviations \n","        sample_abbr = {\n","                        \"$\" : \" dollar \",\n","                        \"€\" : \" euro \",\n","                        \"4ao\" : \"for adults only\",\n","                        \"a.m\" : \"before midday\",\n","                        \"a3\" : \"anytime anywhere anyplace\",\n","                        \"aamof\" : \"as a matter of fact\",\n","                        \"acct\" : \"account\",\n","                        \"adih\" : \"another day in hell\",\n","                        \"afaic\" : \"as far as i am concerned\",\n","                        \"afaict\" : \"as far as i can tell\",\n","                        \"afaik\" : \"as far as i know\",\n","                        \"afair\" : \"as far as i remember\",\n","                        \"afk\" : \"away from keyboard\",\n","                        \"app\" : \"application\",\n","                        \"approx\" : \"approximately\",\n","                        \"apps\" : \"applications\",\n","                        \"asap\" : \"as soon as possible\",\n","                        \"asl\" : \"age, sex, location\",\n","                        \"atk\" : \"at the keyboard\",\n","                        \"ave.\" : \"avenue\",\n","                        \"aymm\" : \"are you my mother\",\n","                        \"ayor\" : \"at your own risk\", \n","                        \"b&b\" : \"bed and breakfast\",\n","                        \"b+b\" : \"bed and breakfast\",\n","                        \"b.c\" : \"before christ\",\n","                        \"b2b\" : \"business to business\",\n","                        \"b2c\" : \"business to customer\",\n","                        \"b4\" : \"before\",\n","                        \"b4n\" : \"bye for now\",\n","                        \"b@u\" : \"back at you\",\n","                        \"bae\" : \"before anyone else\",\n","                        \"bak\" : \"back at keyboard\",\n","                        \"bbbg\" : \"bye bye be good\",\n","                        \"bbc\" : \"british broadcasting corporation\",\n","                        \"bbias\" : \"be back in a second\",\n","                        \"bbl\" : \"be back later\",\n","                        \"bbs\" : \"be back soon\",\n","                        \"be4\" : \"before\",\n","                        \"bfn\" : \"bye for now\",\n","                        \"blvd\" : \"boulevard\",\n","                        \"bout\" : \"about\",\n","                        \"brb\" : \"be right back\",\n","                        \"bros\" : \"brothers\",\n","                        \"brt\" : \"be right there\",\n","                        \"bsaaw\" : \"big smile and a wink\",\n","                        \"btw\" : \"by the way\",\n","                        \"bwl\" : \"bursting with laughter\",\n","                        \"c/o\" : \"care of\",\n","                        \"cet\" : \"central european time\",\n","                        \"cf\" : \"compare\",\n","                        \"cia\" : \"central intelligence agency\",\n","                        \"csl\" : \"can not stop laughing\",\n","                        \"cu\" : \"see you\",\n","                        \"cul8r\" : \"see you later\",\n","                        \"cv\" : \"curriculum vitae\",\n","                        \"cwot\" : \"complete waste of time\",\n","                        \"cya\" : \"see you\",\n","                        \"cyt\" : \"see you tomorrow\",\n","                        \"dae\" : \"does anyone else\",\n","                        \"dbmib\" : \"do not bother me i am busy\",\n","                        \"diy\" : \"do it yourself\",\n","                        \"dm\" : \"direct message\",\n","                        \"dwh\" : \"during work hours\",\n","                        \"e123\" : \"easy as one two three\",\n","                        \"eet\" : \"eastern european time\",\n","                        \"eg\" : \"example\",\n","                        \"embm\" : \"early morning business meeting\",\n","                        \"encl\" : \"enclosed\",\n","                        \"encl.\" : \"enclosed\",\n","                        \"etc\" : \"and so on\",\n","                        \"faq\" : \"frequently asked questions\",\n","                        \"fawc\" : \"for anyone who cares\",\n","                        \"fb\" : \"facebook\",\n","                        \"fc\" : \"fingers crossed\",\n","                        \"fig\" : \"figure\",\n","                        \"fimh\" : \"forever in my heart\", \n","                        \"ft.\" : \"feet\",\n","                        \"ft\" : \"featuring\",\n","                        \"ftl\" : \"for the loss\",\n","                        \"ftw\" : \"for the win\",\n","                        \"fwiw\" : \"for what it is worth\",\n","                        \"fyi\" : \"for your information\",\n","                        \"g9\" : \"genius\",\n","                        \"gahoy\" : \"get a hold of yourself\",\n","                        \"gal\" : \"get a life\",\n","                        \"gcse\" : \"general certificate of secondary education\",\n","                        \"gfn\" : \"gone for now\",\n","                        \"gg\" : \"good game\",\n","                        \"gl\" : \"good luck\",\n","                        \"glhf\" : \"good luck have fun\",\n","                        \"gmt\" : \"greenwich mean time\",\n","                        \"gmta\" : \"great minds think alike\",\n","                        \"gn\" : \"good night\",\n","                        \"g.o.a.t\" : \"greatest of all time\",\n","                        \"goat\" : \"greatest of all time\",\n","                        \"goi\" : \"get over it\",\n","                        \"gps\" : \"global positioning system\",\n","                        \"gr8\" : \"great\",\n","                        \"gratz\" : \"congratulations\",\n","                        \"gyal\" : \"girl\",\n","                        \"h&c\" : \"hot and cold\",\n","                        \"hp\" : \"horsepower\",\n","                        \"hr\" : \"hour\",\n","                        \"hrh\" : \"his royal highness\",\n","                        \"ht\" : \"height\",\n","                        \"ibrb\" : \"i will be right back\",\n","                        \"ic\" : \"i see\",\n","                        \"icq\" : \"i seek you\",\n","                        \"icymi\" : \"in case you missed it\",\n","                        \"idc\" : \"i do not care\",\n","                        \"idgadf\" : \"i do not give a damn fuck\",\n","                        \"idgaf\" : \"i do not give a fuck\",\n","                        \"idk\" : \"i do not know\",\n","                        \"ie\" : \"that is\",\n","                        \"i.e\" : \"that is\",\n","                        \"ifyp\" : \"i feel your pain\",\n","                        \"IG\" : \"instagram\",\n","                        \"iirc\" : \"if i remember correctly\",\n","                        \"ilu\" : \"i love you\",\n","                        \"ily\" : \"i love you\",\n","                        \"imho\" : \"in my humble opinion\",\n","                        \"imo\" : \"in my opinion\",\n","                        \"imu\" : \"i miss you\",\n","                        \"iow\" : \"in other words\",\n","                        \"irl\" : \"in real life\",\n","                        \"j4f\" : \"just for fun\",\n","                        \"jic\" : \"just in case\",\n","                        \"jk\" : \"just kidding\",\n","                        \"jsyk\" : \"just so you know\",\n","                        \"l8r\" : \"later\",\n","                        \"lb\" : \"pound\",\n","                        \"lbs\" : \"pounds\",\n","                        \"ldr\" : \"long distance relationship\",\n","                        \"lmao\" : \"laugh my ass off\",\n","                        \"lmfao\" : \"laugh my fucking ass off\",\n","                        \"lol\" : \"laughing out loud\",\n","                        \"ltd\" : \"limited\",\n","                        \"ltns\" : \"long time no see\",\n","                        \"m8\" : \"mate\",\n","                        \"mf\" : \"motherfucker\",\n","                        \"mfs\" : \"motherfuckers\",\n","                        \"mfw\" : \"my face when\",\n","                        \"mofo\" : \"motherfucker\",\n","                        \"mph\" : \"miles per hour\",\n","                        \"mr\" : \"mister\",\n","                        \"mrw\" : \"my reaction when\",\n","                        \"ms\" : \"miss\",\n","                        \"mte\" : \"my thoughts exactly\",\n","                        \"nagi\" : \"not a good idea\",\n","                        \"nbc\" : \"national broadcasting company\",\n","                        \"nbd\" : \"not big deal\",\n","                        \"nfs\" : \"not for sale\",\n","                        \"ngl\" : \"not going to lie\",\n","                        \"nhs\" : \"national health service\",\n","                        \"nrn\" : \"no reply necessary\",\n","                        \"nsfl\" : \"not safe for life\",\n","                        \"nsfw\" : \"not safe for work\",\n","                        \"nth\" : \"nice to have\",\n","                        \"nvr\" : \"never\",\n","                        \"nyc\" : \"new york city\",\n","                        \"oc\" : \"original content\",\n","                        \"og\" : \"original\",\n","                        \"ohp\" : \"overhead projector\",\n","                        \"oic\" : \"oh i see\",\n","                        \"omdb\" : \"over my dead body\",\n","                        \"omg\" : \"oh my god\",\n","                        \"omw\" : \"on my way\",\n","                        \"p.a\" : \"per annum\",\n","                        \"p.m\" : \"after midday\",\n","                        \"pm\" : \"prime minister\",\n","                        \"poc\" : \"people of color\",\n","                        \"pov\" : \"point of view\",\n","                        \"pp\" : \"pages\",\n","                        \"ppl\" : \"people\",\n","                        \"prw\" : \"parents are watching\",\n","                        \"ps\" : \"postscript\",\n","                        \"pt\" : \"point\",\n","                        \"ptb\" : \"please text back\",\n","                        \"pto\" : \"please turn over\",\n","                        \"qpsa\" : \"what happens\", #\"que pasa\",\n","                        \"ratchet\" : \"rude\",\n","                        \"rbtl\" : \"read between the lines\",\n","                        \"rlrt\" : \"real life retweet\", \n","                        \"rofl\" : \"rolling on the floor laughing\",\n","                        \"roflol\" : \"rolling on the floor laughing out loud\",\n","                        \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n","                        \"rt\" : \"retweet\",\n","                        \"ruok\" : \"are you ok\",\n","                        \"sfw\" : \"safe for work\",\n","                        \"sk8\" : \"skate\",\n","                        \"smh\" : \"shake my head\",\n","                        \"sq\" : \"square\",\n","                        \"srsly\" : \"seriously\", \n","                        \"ssdd\" : \"same stuff different day\",\n","                        \"tbh\" : \"to be honest\",\n","                        \"tbs\" : \"tablespooful\",\n","                        \"tbsp\" : \"tablespooful\",\n","                        \"tfw\" : \"that feeling when\",\n","                        \"thks\" : \"thank you\",\n","                        \"tho\" : \"though\",\n","                        \"thx\" : \"thank you\",\n","                        \"tia\" : \"thanks in advance\",\n","                        \"til\" : \"today i learned\",\n","                        \"tl;dr\" : \"too long i did not read\",\n","                        \"tldr\" : \"too long i did not read\",\n","                        \"tmb\" : \"tweet me back\",\n","                        \"tntl\" : \"trying not to laugh\",\n","                        \"ttyl\" : \"talk to you later\",\n","                        \"u\" : \"you\",\n","                        \"u2\" : \"you too\",\n","                        \"u4e\" : \"yours for ever\",\n","                        \"utc\" : \"coordinated universal time\",\n","                        \"w/\" : \"with\",\n","                        \"w/o\" : \"without\",\n","                        \"w8\" : \"wait\",\n","                        \"wassup\" : \"what is up\",\n","                        \"wb\" : \"welcome back\",\n","                        \"wtf\" : \"what the fuck\",\n","                        \"wtg\" : \"way to go\",\n","                        \"wtpa\" : \"where the party at\",\n","                        \"wuf\" : \"where are you from\",\n","                        \"wuzup\" : \"what is up\",\n","                        \"wywh\" : \"wish you were here\",\n","                        \"yd\" : \"yard\",\n","                        \"ygtr\" : \"you got that right\",\n","                        \"ynk\" : \"you never know\",\n","                        \"zzz\" : \"sleeping bored and tired\"\n","                        }\n","            \n","        sample_typos_slang_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_typos_slang.keys()) + r')(?!\\w)')\n","        sample_acronyms_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_acronyms.keys()) + r')(?!\\w)')\n","        sample_abbr_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_abbr.keys()) + r')(?!\\w)')\n","        \n","        text = sample_typos_slang_pattern.sub(lambda x: sample_typos_slang[x.group()], text)\n","        text = sample_acronyms_pattern.sub(lambda x: sample_acronyms[x.group()], text)\n","        text = sample_abbr_pattern.sub(lambda x: sample_abbr[x.group()], text)\n","        \n","        return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XinQSdMDz4eD"},"source":["train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: informalAbbreviations_clean(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lo81m5fbNdod"},"source":["!pip install autocorrect"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KZ37br_Zpozn"},"source":["### Spelling Correction"]},{"cell_type":"code","metadata":{"id":"QH_NcAUu1PGW"},"source":["def spelling_correction(text):\n","  \"\"\"\n","    Here apply Textblob to correct the spelling\n","  \"\"\"\n","  repeat_pattern = re.compile(r'(\\w)\\1*')\n","  match_substitution = r'\\1'\n","  new_word = repeat_pattern.sub(match_substitution,text)\n","  spell = Speller(lang='en')  \n","  return spell(new_word)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iCyIBdbI1JjV"},"source":["train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: spelling_correction(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GetVuAwTr7rJ"},"source":["### Converting some emojis text to emojis"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9RTfSzbKhbx5","executionInfo":{"status":"ok","timestamp":1619563051616,"user_tz":-120,"elapsed":825,"user":{"displayName":"Nancy Hassan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfzroUWqthIy7jUUOd9jfaIhO9mtFnKpf8FI7FpQ=s64","userId":"08005434482666482410"}},"outputId":"927f22d1-87d9-4981-a546-ac9407e02249"},"source":["\n","def emojiTransalte(Text):\n","   dict_emot= { ':-)'  : b'\\xf0\\x9f\\x98\\x8a'.decode('utf-8'),\n","                ':)'   : b'\\xf0\\x9f\\x98\\x8a'.decode('utf-8'),\n","                '=)'   : b'\\xf0\\x9f\\x98\\x8a'.decode('utf-8'),  # Smile or happy\n","                ':-D'  : b'\\xf0\\x9f\\x98\\x83'.decode('utf-8'),\n","                ':D'   : b'\\xf0\\x9f\\x98\\x83'.decode('utf-8'),\n","                '=D'   : b'\\xf0\\x9f\\x98\\x83'.decode('utf-8'),  # Big smile\n","                '>:-(' : b'\\xF0\\x9F\\x98\\xA0'.decode('utf-8'),\n","                ':@'   : b'\\xF0\\x9F\\x98\\xA0'.decode('utf-8')   # Angry face\n","    }\n","    return dict_emot[text]"],"execution_count":3,"outputs":[{"output_type":"stream","text":["😊\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Gww7gT_PgHE0"},"source":["train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: emojiTransalte(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lQptCVO3sHbH"},"source":["### Remove non ascii charcters"]},{"cell_type":"code","metadata":{"id":"hWVtKJQOr9gX"},"source":["def remove_non_ascii(text):\n","\n","    \"\"\"\n","        Remove non-ASCII characters \n","    \"\"\"\n","\n","    return re.sub(r'[^\\x00-\\x7f]',r'', text) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"12PeMtcer9qS"},"source":["train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_non_ascii(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kDA5QDoKsN9c"},"source":["### Remove any other special characters"]},{"cell_type":"code","metadata":{"id":"aK4HiF1ctyaM"},"source":["def remove_special_characters(text):\n","    \"\"\"\n","        Remove special special characters, including symbols, emojis, and other graphic characters\n","    \"\"\"\n","    emoji_pattern = re.compile(\n","        '['\n","        u'\\U0001F600-\\U0001F64F'  # emoticons\n","        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n","        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n","        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n","        u'\\U00002702-\\U000027B0'\n","        u'\\U000024C2-\\U0001F251'\n","        ']+',\n","        flags=re.UNICODE)\n","    return emoji_pattern.sub(r'', text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jJ-nIi9lt4q9"},"source":["train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_special_characters(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DdFa7rJ4scmX"},"source":["### Remove punctuation"]},{"cell_type":"code","metadata":{"id":"Cd5lpBLMt7gF"},"source":["def remove_punct(text):\n","    \"\"\"\n","        Remove the punctuation\n","    \"\"\"\n","    return text.translate(str.maketrans('', '', string.punctuation))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q3ba7b_qvC_y"},"source":["train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_punct(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YfOjDB_lhZRL"},"source":["\n","# Text Preprocessing\n"]},{"cell_type":"markdown","metadata":{"id":"1bAT3-7Xtw7_"},"source":["### **Tokenization**"]},{"cell_type":"code","metadata":{"id":"NSlKvw5IhYAp"},"source":["!pip install nltk\n","from nltk.tokenize import word_tokenize\n","\n","train_df[\"tokenized\"] = train_df[\"text_clean\"].apply(word_tokenize)\n","train_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iHM4_fkLhOE9"},"source":["\n","### **Remove stop word**"]},{"cell_type":"code","metadata":{"id":"S57840OahNVx"},"source":["nltk.download(\"stopwords\")\n","from nltk.corpus import stopwords\n","\n","stop = set(stopwords.words('english'))\n","train_df['stopwords_removed'] = train_df[\"tokenized\"].apply(lambda x: [word for word in x if word not in stop])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QEAsIAOmjplO"},"source":["### **Part of Speech Tagging (POS Tagging):**\n","\n","Part of speech tagging (POS tagging) distinguishes the part of speech (noun, verb, adjective, and etc.) of each word in the text."]},{"cell_type":"code","metadata":{"id":"X93d9jNBjpKw"},"source":["from nltk.corpus import wordnet\n","from nltk.corpus import brown\n","\n","wordnet_map = {\"N\":wordnet.NOUN, \n","               \"V\":wordnet.VERB, \n","               \"J\":wordnet.ADJ, \n","               \"R\":wordnet.ADV\n","              }\n","    \n","train_sents = brown.tagged_sents(categories='news')\n","t0 = nltk.DefaultTagger('NN')\n","t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n","t2 = nltk.BigramTagger(train_sents, backoff=t1)\n","\n","def pos_tag_wordnet(text, pos_tag_type=\"pos_tag\"):\n","    \"\"\"\n","        Create pos_tag with wordnet format\n","    \"\"\"\n","    pos_tagged_text = t2.tag(text)\n","    \n","    # map the pos tagging output with wordnet output \n","    pos_tagged_text = [(word, wordnet_map.get(pos_tag[0])) if pos_tag[0] in wordnet_map.keys() else (word, wordnet.NOUN) for (word, pos_tag) in pos_tagged_text ]\n","    return pos_tagged_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PsxJo-ZMjzed"},"source":["train_df['pos_tag'] = train_df['stopwords_removed'].apply(lambda x: pos_tag_wordnet(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ovWC2bDEj8BS"},"source":["### **Lemmatization:**\n","\n","Lemmatization is the task of determining that two words have the same root, despite their surface differences. The words am, are, and is have the shared lemma be; the words dinner and dinners both have the lemma dinner. \n","Lemmatizing each of these forms to the same lemma will let us ﬁnd all mentions of words in Russian like Moscow."]},{"cell_type":"code","metadata":{"id":"Y-AQijDVkNRs"},"source":["\n","lemmatizer = WordNetLemmatizer()\n","\n","train_df['lemmatize_word_w_pos'] = train_df['pos_tag'].apply(lambda x: lemmatize_word(x))\n","train_df['lemmatize_word_w_pos'] = train_df['lemmatize_word_w_pos'].apply(lambda x: [word for word in x if word not in stop]) # double check to remove stop words\n","train_df['lemmatize_text'] = [' '.join(map(str, l)) for l in train_df['lemmatize_word_w_pos']] # join back to text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9UtI18Fft6pA"},"source":["### **Saving Dataset**"]},{"cell_type":"code","metadata":{"id":"oPgGeR4j9NKz"},"source":["train_df.to_csv('cleanedTwitter.csv')"],"execution_count":null,"outputs":[]}]}